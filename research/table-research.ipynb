{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29706a1c",
   "metadata": {},
   "source": [
    "# Table Search\n",
    "## Problem description\n",
    "1. Table detection is handled by PyMuPDF4LLM\n",
    "2. The tables may be multipage\n",
    "3. The tables may be very large and/or sparse\n",
    "\n",
    "## A separate chunking strategy for tables\n",
    "The starting point for our approach is:\n",
    "(Financial Report Chunking for Effective Retrieval Augmented Generation)[https://arxiv.org/abs/2402.05131]\n",
    "\n",
    "The starting point of the approach taken (of interested is marked in **bold**)\n",
    "\"The steps to generate element-based chunks are:\n",
    "– if the element text length is smaller than 2,048 characters, a merge with the\n",
    "following element is attempted\n",
    "– iteratively, element texts are merged following the step above till either the\n",
    "desired length is achieved, without breaking the element\n",
    "– if a title element is found, a new chunk is started\n",
    "– **if a table element is found, a new chunk is started, preserving the entire table**\"\n",
    "\n",
    "We will not enforce the step **if a table element is found, a new chunk is started, preserving the entire table**.\n",
    "Rather we will use structural seperators to encourage this behavior.\n",
    "\n",
    "In addition, the files contain large table, potentially spanning multiple pages. The next section will look at a strategy based on limitations of the general chunking strategy, embedding models, rerankers and LLMs.\n",
    "\n",
    "### Data enrichment for effective retrieval\n",
    "Directly generating an embedding on tables may be undesirable.\n",
    "\n",
    "For any table:\n",
    "\n",
    "\n",
    "The task description will be placed before the table in the prompt (Table Meets LLM: Can Large Language Models Understand Structured Table Data)[https://arxiv.org/pdf/2305.13062]\n",
    "\n",
    "```prompt\n",
    "# Task\n",
    "Please give a table description in plain text.\n",
    "\n",
    "The description should include:\n",
    "1. Creating a table caption\n",
    "2. Describe column and row headers\n",
    "\n",
    "# Table\n",
    "{insert table}\n",
    "```\n",
    "\n",
    "### Distinction between small, medium and large tables\n",
    "The upper bound for the chunk size is set by by recursive text splitter, which is set to a MAX_CHUNK_SIZE OF 1000 characters (note: technically the langchain implementation of recursive text splitter may not always respect this upper bound). \n",
    "\n",
    "As our strategy we opted for a different strategy depending on the size of a table. \n",
    "\n",
    "1. For small tables, only enriched with contextual information for better retrieval. Table embedded\n",
    "2. For medium tables, contextual information + table metadata. \n",
    "3. For large tables, excluded as LLMs performance degrades significantly.\n",
    "\n",
    "Therefore, a small table will be that will fit in the smallest chunk size. The approach for small tables is to enricht with metadata for improved retrieval.\n",
    "\n",
    "For tables larger than MAX_CHUNK_SIZE, in a naive strategy the table will be split up. This should be prevented, because:\n",
    "1. Adds additional noisy chunks\n",
    "2. Later chunks do not have header metadata\n",
    "\n",
    "On average 1 token is about 4 characters in normal text. However, this often does not hold true for tables, especially with numeric data (e.g. 1,0 or 1.0 is typically 3 tokens). Assuming the worse case the lower bound of 1 token = 1 table character, would mean a small table of 1000 characters will be 1000 tokens. The small table will also fit well within the bounds of both the embedding and LLMs with enriched data will fit well within Gemini-embedding-001, which has the smallest context window of 2048 tokens. \n",
    "\n",
    "| Embedding Model                | Maximum Input Token Length |\n",
    "|--------------------------------|-----------------------------|\n",
    "| gemini-embedding-001 (GA)      | **2048** tokens |\n",
    "| text-embedding-3-large         | **8191** tokens |\n",
    "| Qwen-3-Embedding-8B            | **32,000** tokens |\n",
    "\n",
    "Tables larger than 1000 character will no longer fit in one chunk (based on our predefined chunk strategy). The rerankers have similar limitations. For the reranker, the preferred approach is to be able to put the entire table into the reranker. The MAX_TABLE_SIZE_RERANKER = 4096 tokens will serve as an upper bound for table size, which can be evaluated through the reranker.\n",
    "\n",
    "| Rerankers | Maximum Input Token Length |\n",
    "| --- | --- |\n",
    "| Qwen3-Reranker-8B | 32,000 tokens |\n",
    "| rerank-v3.5 | 4096 tokens |\n",
    "\n",
    "Note: strictly speaking the context window is not a hard limitation. Alternative approaches exist.\n",
    "1. Document truncation\n",
    "2. Max score of documents chunks\n",
    "(docs - cohere 3.5 reranker)[https://docs.cohere.com/v2/docs/reranking-best-practices]\n",
    "However, the ability of LLMs to handle tables also plays a role.\n",
    "\n",
    "In addition limitation of the reranker, the capabilities of the LLMs, shows a performance drop for gpt-4o-mini between 1K tokens (~50% column average, ~40% column substraction) --> 4K tokens (~35% column average, 41% column substraction). Based on (livebench)[https://livebench.ai/#/], recent models seem to be significantly better at handling long context. \n",
    "1. Possible to retrieve multiple large tables with a query\n",
    "2. The app has chat history a large table will explode the context window\n",
    "Therefore, a MAX_TABLE_SIZE_LLM = 4K characters is a good cut-off \n",
    "\n",
    "Lastly, the LLM may need to handle multiple tables.\n",
    "The upper bound for medium-sized table is chosen based on the capabilities of LLMs, which show a significant drop off after 4k (How well do LLMs reason over tabular data, really?)[https://arxiv.org/pdf/2505.07453]. The study was performed on less recent models.\n",
    "\n",
    "| LLM                       | Context Window Size |\n",
    "|---------------------------|---------------------|\n",
    "| GPT-4o Mini               | 128,000 tokens      |\n",
    "| GPT-4o                    | 128,000 tokens      |\n",
    "| GPT-5                     | 400,000 tokens      |\n",
    "| Gemini 2.5 Pro            | 1,000,000 tokens    |\n",
    "| Grok 4                    | 256,000 tokens      |\n",
    "\n",
    "\n",
    "\n",
    "Based on the aforementioned, the following table strategy was devised.\n",
    "\n",
    "| Category |Table Size (characters)                | Relative noise (not based on anything) |Action\n",
    "|--------  |-------------------------------------  |----------------------------------------| --- |\n",
    "| Small    | size < 1000 (MAX_CHUNK_SIZE)          | low                                    | Enrich for retrieval |\n",
    "| Medium   | 1000 < size < 4000 (MAX_TABLE_SIZE_RERANKER && MAX_TABLE_SIZE_LLM)| medium                                 | Enrich for retrieval && replace table with description for retrieval |\n",
    "| Large    | size > 4000                           | high                                   | Exclude |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0872c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED TO MAIN PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b14be7",
   "metadata": {},
   "source": [
    "# Storage of table summaries (and equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bases is the recursive text splitter.\n",
    "# to simulate the impact on the real document pipeline, we will split the document into 1000 with 200\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "MAGIC_CHUNK_SIZE = 1000\n",
    "MAGIC_CHUNK_OVERLAP = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=MAGIC_CHUNK_SIZE,\n",
    "    chunk_overlap=MAGIC_CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "docs  = text_splitter.transform_documents([Document(page_content=content)])\n",
    "\n",
    "docs = docs[1261:1356]\n",
    "\n",
    "# for idx, doc in enumerate(docs):\n",
    "#     if \"<table>\" in doc.page_content:\n",
    "#        print(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import uuid\n",
    "from langchain.storage import InMemoryByteStore\n",
    "# from langchain.storage import \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi-vector-test-0\", embedding_function=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=os.environ[\"AZURE_OPENAI_API_KEY\"], azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"], api_version=os.environ[\"AZURE_TEXT_EMBEDDING_LARGE_VERSION\"])\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"parent_doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "\n",
    "sub_docs = []\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=MAGIC_CHUNK_SIZE/2)\n",
    "for i, doc in enumerate(docs):\n",
    "    _parent_doc_id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _sub_doc in _sub_docs:\n",
    "        _sub_doc.metadata[id_key] = _parent_doc_id\n",
    "    sub_docs.extend(_sub_docs)\n",
    "\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda19f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: table\n",
      "Content: \n",
      "            | Product | Q1 Sales | Q2 Sales | Q3 Sales | Q4 Sales |\n",
      "            |---------|----------|----------|----------|----------|\n",
      "            | Widget A| 1000     | 1200     | 1100     | 1300  ...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.schema import Document\n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize your components\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi-vector-test-0\", \n",
    "    embedding_function=AzureOpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\", \n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"], \n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"], \n",
    "        api_version=os.environ[\"AZURE_TEXT_EMBEDDING_LARGE_VERSION\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Initialize LLM for generating summaries\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_SWEDEN\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_SWEDEN\"],\n",
    "    api_version=os.environ[\"AZURE_GPT_5_VERSION\"],\n",
    "    deployment_name=\"gpt-5-chat\"\n",
    ")\n",
    "\n",
    "def generate_table_summary(table_content: str) -> str:\n",
    "    \"\"\"Generate a summary of table content using LLM\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please provide a concise summary of the following table that captures:\n",
    "    1. What the table is about (main topic/subject)\n",
    "    2. Key columns and their types of data\n",
    "    3. Important patterns, trends, or notable information\n",
    "    4. The table's purpose or what insights it provides\n",
    "    \n",
    "    Table content:\n",
    "    {table_content}\n",
    "    \n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def add_tables_with_summaries(tables_data):\n",
    "    \"\"\"\n",
    "    Add tables to the retriever with their summaries\n",
    "    \n",
    "    Args:\n",
    "        tables_data: List of dictionaries with 'content' and optional 'metadata'\n",
    "                    Example: [{'content': 'table_html_or_text', 'metadata': {'source': 'file.pdf'}}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate unique IDs for each table\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables_data]\n",
    "    \n",
    "    # Create table documents\n",
    "    table_docs = []\n",
    "    table_summaries = []\n",
    "    \n",
    "    for i, table_data in enumerate(tables_data):\n",
    "        table_content = table_data['content']\n",
    "        table_metadata = table_data.get('metadata', {})\n",
    "        \n",
    "        # Create the original table document\n",
    "        table_doc = Document(\n",
    "            page_content=table_content,\n",
    "            metadata={**table_metadata, 'type': 'table'}\n",
    "        )\n",
    "        table_docs.append(table_doc)\n",
    "        \n",
    "        # Generate summary for the table\n",
    "        summary = generate_table_summary(table_content)\n",
    "        \n",
    "        # Create summary document that will be embedded\n",
    "        summary_doc = Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                **table_metadata,\n",
    "                id_key: table_ids[i],\n",
    "                'type': 'table_summary',\n",
    "                'original_type': 'table'\n",
    "            }\n",
    "        )\n",
    "        table_summaries.append(summary_doc)\n",
    "    \n",
    "    # Add summaries to vectorstore (these will be searched)\n",
    "    retriever.vectorstore.add_documents(table_summaries)\n",
    "    \n",
    "    # Store original table documents (these will be retrieved)\n",
    "    retriever.docstore.mset(list(zip(table_ids, table_docs)))\n",
    "    \n",
    "    return table_ids\n",
    "\n",
    "# Example usage with your existing docs + tables\n",
    "def process_mixed_documents(docs, tables_data):\n",
    "    \"\"\"Process both regular documents and tables\"\"\"\n",
    "    \n",
    "    # Process regular documents (your existing code)\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "    \n",
    "    sub_docs = []\n",
    "    child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=MAGIC_CHUNK_SIZE//2)\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        _id = doc_ids[i]\n",
    "        _sub_docs = child_text_splitter.split_documents([doc])\n",
    "        for _doc in _sub_docs:\n",
    "            _doc.metadata[id_key] = _id\n",
    "        sub_docs.extend(_sub_docs)\n",
    "    \n",
    "    # Add document chunks to vectorstore\n",
    "    retriever.vectorstore.add_documents(sub_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "    \n",
    "    # Process tables with summaries\n",
    "    table_ids = add_tables_with_summaries(tables_data)\n",
    "    \n",
    "    return doc_ids, table_ids\n",
    "\n",
    "# Example of how to use it\n",
    "if __name__ == \"__main__\":\n",
    "    # Example table data\n",
    "    tables_data = [\n",
    "        {\n",
    "            'content': \"\"\"\n",
    "            | Product | Q1 Sales | Q2 Sales | Q3 Sales | Q4 Sales |\n",
    "            |---------|----------|----------|----------|----------|\n",
    "            | Widget A| 1000     | 1200     | 1100     | 1300     |\n",
    "            | Widget B| 800      | 900      | 950      | 1000     |\n",
    "            | Widget C| 1200     | 1100     | 1250     | 1400     |\n",
    "            \"\"\",\n",
    "            'metadata': {'source': 'sales_report.pdf', 'page': 5}\n",
    "        },\n",
    "        {\n",
    "            'content': \"\"\"\n",
    "            | Employee | Department | Salary | Years |\n",
    "            |----------|------------|--------|-------|\n",
    "            | John Doe | Engineering| 75000  | 3     |\n",
    "            | Jane Smith| Marketing | 65000  | 2     |\n",
    "            | Bob Wilson| Sales     | 70000  | 5     |\n",
    "            \"\"\",\n",
    "            'metadata': {'source': 'hr_report.pdf', 'page': 12}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process documents and tables\n",
    "    doc_ids, table_ids = process_mixed_documents(docs, tables_data)\n",
    "    \n",
    "    # Now you can retrieve based on table summaries\n",
    "    query = \"What are the quarterly sales figures for different products?\"\n",
    "    results = retriever.invoke(query, config={\"fetch_k\": 1})\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"Type: {result.metadata.get('type', 'document')}\")\n",
    "        print(f\"Content: {result.page_content[:200]}...\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2654d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equation_summary(equation_content: str) -> str:\n",
    "    \"\"\"Generate a comprehensive summary of mathematical equation\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please provide a detailed summary of the following mathematical equation that includes:\n",
    "    1. What the equation represents (physical law, mathematical relationship, etc.)\n",
    "    2. Key variables and their meanings\n",
    "    3. The mathematical domain or field it belongs to\n",
    "    4. What the equation is used to calculate or prove\n",
    "    5. Any important mathematical properties or constraints\n",
    "    6. Common applications or contexts where this equation is used\n",
    "    \n",
    "    Equation: {equation_content}\n",
    "    \n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def add_equations_with_summaries(equations_data):\n",
    "    \"\"\"Add equations with their summaries for better retrieval\"\"\"\n",
    "    \n",
    "    equation_ids = [str(uuid.uuid4()) for _ in equations_data]\n",
    "    equation_docs = []\n",
    "    equation_summaries = []\n",
    "    \n",
    "    for i, eq_data in enumerate(equations_data):\n",
    "        equation_content = eq_data['content']  # LaTeX, MathML, or text representation\n",
    "        equation_metadata = eq_data.get('metadata', {})\n",
    "        \n",
    "        # Create original equation document\n",
    "        equation_doc = Document(\n",
    "            page_content=equation_content,\n",
    "            metadata={**equation_metadata, 'type': 'equation'}\n",
    "        )\n",
    "        equation_docs.append(equation_doc)\n",
    "        \n",
    "        # Generate semantic summary\n",
    "        summary = generate_equation_summary(equation_content)\n",
    "        \n",
    "        # Create summary document for embedding\n",
    "        summary_doc = Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                **equation_metadata,\n",
    "                id_key: equation_ids[i],\n",
    "                'type': 'equation_summary',\n",
    "                'original_type': 'equation'\n",
    "            }\n",
    "        )\n",
    "        equation_summaries.append(summary_doc)\n",
    "    \n",
    "    # Add summaries to vectorstore (searchable)\n",
    "    retriever.vectorstore.add_documents(equation_summaries)\n",
    "    \n",
    "    # Store original equations (retrievable)\n",
    "    retriever.docstore.mset(list(zip(equation_ids, equation_docs)))\n",
    "    \n",
    "    return equation_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5ee1a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.vectorstore.similarity_search(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb6f3bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.invoke(\"justice breyer\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048072e",
   "metadata": {},
   "source": [
    "# Custom splitter\n",
    "1. Try to split on different media types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regulation-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
