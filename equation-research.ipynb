{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a31b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF 1.26.3: Python bindings for the MuPDF 1.26.3 library (rebased implementation).\n",
      "Python 3.13 running on win32 (64-bit).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "\n",
    "print(pymupdf.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a5d82",
   "metadata": {},
   "source": [
    "# Problem\n",
    "* Based on https://github.com/pymupdf/PyMuPDF/discussions/763\n",
    "* Start with looking at PyMuPDF PDF engine and its capabilities\n",
    "* No build in equation detection, especially problematic for text based equation\n",
    "\n",
    "# Pipeline\n",
    "1. Equation detection\n",
    "    * (option) verify detection\n",
    "2. Equation conversion latex equation\n",
    "    * (option) verify conversion\n",
    "3. Replacing original equation with latex equation\n",
    "\n",
    "\n",
    "## Equation detection\n",
    "### 1. heuristic based equation detection\n",
    "In our case code examples are not a problem:\n",
    "\n",
    "Yes, exactly!\n",
    "In PDF, text is just text. The PDF specification contains nothing to sub-divide different kinds of text. Equations are also text and be coded in any font, can be italic, or normal, mono-spaced of proportional, serifed or sans-serifed.\n",
    "Also note that the equation symbol appears in program code listings a lot - PyMuPDF.pdf is full of such examples.\n",
    "\n",
    "So I would say, that you have to develop your own way of recognizing equations ... and whatever you will develop, may not work with the next PDF example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910be774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9add5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# test_folder = os.path.join(\"data\", \"raw\", \"test-data\", \"equation-examples\")\n",
    "\n",
    "# for file_name in os.listdir(test_folder):\n",
    "#     doc = pymupdf.open(os.path.join(test_folder, file_name))\n",
    "#     for page in doc:\n",
    "#         # print(page.get_text(\"html\"))\n",
    "#         text_in_dict = page.get_text(\"dict\", flags=0)\n",
    "#         # tables = page.find_tables()\n",
    "#         # pprint.pprint(blocks)\n",
    "#         with open(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\", f'{file_name}-page_{page.number}.json'), 'w') as f:\n",
    "#             json.dump(text_in_dict, f)\n",
    "#         # page.get_pixmap().save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea \n",
    "# is if you can detect the equation\n",
    "# then you can just image multimodal, somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679555a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING THIS IS AI SLOP\n",
    "import pymupdf\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Heuristics Configuration ---\n",
    "\n",
    "# 1. Define characters that strongly indicate mathematical notation\n",
    "WEAK_MATH_SYMBOLS = {'×', '+', '−'}\n",
    "STRONG_MATH_SYMBOLS = {'√', '∑', '=', '¼', '∂', '∫', '≥', '≤', '≠', '�', 'Þ'}\n",
    "\n",
    "# 2. Define scoring weights for different features\n",
    "SCORING_WEIGHTS = {\n",
    "    'is_strong_math_symbol': 15,\n",
    "    'is_weak_math_symbol': 3,\n",
    "    'is_large_symbol': 3,\n",
    "    'is_subscript': 3,\n",
    "    'is_superscript': 3,\n",
    "    'is_italic': 1,\n",
    "    'is_largely_alphabetic': 5\n",
    "}\n",
    "\n",
    "# 3. Threshold for a line to be considered part of an equation\n",
    "LINE_SCORE_THRESHOLD = 9\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def merge_bboxes(bboxes):\n",
    "    \"\"\"Merges a list of bounding boxes into a single bounding box.\"\"\"\n",
    "    if not bboxes:\n",
    "        return None\n",
    "    min_x0 = min(b[0] for b in bboxes)\n",
    "    min_y0 = min(b[1] for b in bboxes)\n",
    "    max_x1 = max(b[2] for b in bboxes)\n",
    "    max_y1 = max(b[3] for b in bboxes)\n",
    "    return (min_x0, min_y0, max_x1, max_y1)\n",
    "\n",
    "def get_dominant_line_properties(line):\n",
    "    \"\"\"Calculates the most common font size and baseline for a line.\"\"\"\n",
    "    if not line['spans']:\n",
    "        return 0, 0, False\n",
    "        \n",
    "    baselines = [round(s['bbox'][3], 2) for s in line['spans']]\n",
    "    sizes = [round(s['size'], 2) for s in line['spans']]\n",
    "    fonts = [s['font'] for s in line['spans']]\n",
    "    \n",
    "    dominant_size = collections.Counter(sizes).most_common(1)[0][0]\n",
    "    dominant_baseline = collections.Counter(baselines).most_common(1)[0][0]\n",
    "    is_bold_dominant = 'Bold' in collections.Counter(fonts).most_common(1)[0][0]\n",
    "\n",
    "    return dominant_size, dominant_baseline, is_bold_dominant\n",
    "\n",
    "def is_likely_heading_or_prose(line, is_bold_dominant):\n",
    "    \"\"\"\n",
    "    Applies negative heuristics to determine if a line is likely a heading or regular text.\n",
    "    \"\"\"\n",
    "    full_text = \"\".join(span['text'] for span in line['spans']).strip()\n",
    "    \n",
    "    if not full_text:\n",
    "        return False\n",
    "\n",
    "    # Heuristic 1: Starts with a number like \"1.\" or \"A.\"\n",
    "    if re.match(r'^[\\[\\(]?\\d{1,2}[\\.\\)]', full_text):\n",
    "        return True\n",
    "\n",
    "    # Heuristic 2: Line is dominantly bold.\n",
    "    if is_bold_dominant:\n",
    "        return True\n",
    "\n",
    "    # Heuristic 3: High ratio of letters to other characters.\n",
    "    # Equations have a low ratio of letters.\n",
    "    text_no_space = full_text.replace(\" \", \"\")\n",
    "    if not text_no_space:\n",
    "        return False # Empty line\n",
    "        \n",
    "    alpha_chars = sum(1 for char in text_no_space if char.isascii()) #changed is isascii\n",
    "    total_chars = len(text_no_space)\n",
    "    alpha_ratio = alpha_chars / total_chars\n",
    "    \n",
    "    # If over 90% of characters are letters, it's likely prose/heading.\n",
    "    if alpha_ratio > 0.90:\n",
    "        return True\n",
    "    \n",
    "    # print(alpha_ratio)\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_positive_line_score(line, dominant_size, dominant_baseline):\n",
    "    line_score = 0\n",
    "    for span in line['spans']:\n",
    "                if any(char in WEAK_MATH_SYMBOLS for char in span['text']):\n",
    "                    line_score += SCORING_WEIGHTS['is_weak_math_symbol']\n",
    "                if any(char in STRONG_MATH_SYMBOLS for char in span['text']):\n",
    "                    line_score += SCORING_WEIGHTS['is_strong_math_symbol']\n",
    "                if 'Italic' in span['font']:\n",
    "                    line_score += SCORING_WEIGHTS['is_italic']\n",
    "                \n",
    "                height = span['bbox'][3] - span['bbox'][1]\n",
    "                if height > dominant_size * 1.5:\n",
    "                    line_score += SCORING_WEIGHTS['is_large_symbol']\n",
    "                \n",
    "                is_smaller = span['size'] < dominant_size * 0.9\n",
    "                span_baseline = span['bbox'][3]\n",
    "                \n",
    "                if is_smaller and span_baseline > dominant_baseline + 1:\n",
    "                    line_score += SCORING_WEIGHTS['is_subscript']\n",
    "                if is_smaller and span_baseline < dominant_baseline - 2:\n",
    "                    line_score += SCORING_WEIGHTS['is_superscript']\n",
    "\n",
    "    return line_score\n",
    "    \n",
    "\n",
    "\n",
    "# --- Main Detection Logic ---\n",
    "\n",
    "def detect_equations(page_data, verbose=0):\n",
    "    \"\"\"\n",
    "    Detects equations from a page's text dictionary representation.\n",
    "    \"\"\"\n",
    "    math_lines = []\n",
    "    \n",
    "    for block in page_data.get('blocks', []):\n",
    "        if block.get('type', 0) != 0:\n",
    "            continue\n",
    "            \n",
    "        for line in block.get('lines', []):\n",
    "            dominant_size, dominant_baseline, is_bold_dominant = get_dominant_line_properties(line)\n",
    "\n",
    "            if dominant_size == 0:\n",
    "                continue\n",
    "\n",
    "            line_score = calculate_positive_line_score(line, dominant_size, dominant_baseline)\n",
    "\n",
    "            # Check if the line matches heading/prose characteristics.\n",
    "            if is_likely_heading_or_prose(line, is_bold_dominant):\n",
    "                # continue # Skip this line, it's a false positive.\n",
    "                line_score -= SCORING_WEIGHTS['is_largely_alphabetic']\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing line: {[span['text'] for span in line['spans']]} with score {line_score}\")\n",
    "\n",
    "            if line_score >= LINE_SCORE_THRESHOLD:\n",
    "\n",
    "                math_lines.append({\n",
    "                    'score': line_score,\n",
    "                    'bbox': line['bbox'],\n",
    "                    'spans': line['spans']\n",
    "                })\n",
    "\n",
    "    # Stage 3: Clustering (no changes needed here)\n",
    "    if not math_lines:\n",
    "        return []\n",
    "\n",
    "    math_lines.sort(key=lambda l: l['bbox'][1])\n",
    "    clusters = []\n",
    "    current_cluster = [math_lines[0]]\n",
    "    \n",
    "    for i in range(1, len(math_lines)):\n",
    "        prev_line = current_cluster[-1]\n",
    "        current_line = math_lines[i]\n",
    "        vertical_gap = current_line['bbox'][1] - prev_line['bbox'][3]\n",
    "        prev_line_height = prev_line['bbox'][3] - prev_line['bbox'][1]\n",
    "        \n",
    "        if vertical_gap < prev_line_height * 0.5:\n",
    "            current_cluster.append(current_line)\n",
    "        else:\n",
    "            clusters.append(current_cluster)\n",
    "            current_cluster = [current_line]\n",
    "            \n",
    "    clusters.append(current_cluster)\n",
    "    \n",
    "    detected_equations = []\n",
    "    for cluster in clusters:\n",
    "        all_spans = [span for line in cluster for span in line['spans']]\n",
    "        cluster_bbox = merge_bboxes([line['bbox'] for line in cluster])\n",
    "        detected_equations.append({\n",
    "            'bbox': cluster_bbox,\n",
    "            'spans': all_spans\n",
    "        })\n",
    "        \n",
    "    return detected_equations\n",
    "\n",
    "def print_equations(equations):\n",
    "    print(f\"Detected {len(equations)} equations.\\n\")\n",
    "    \n",
    "    for i, eq in enumerate(equations):\n",
    "        eq_text = \"\".join([s['text'] for s in eq['spans']])\n",
    "\n",
    "        print([s['text'] for s in eq['spans']])\n",
    "        print(f\"--- Equation {i+1} ---\")\n",
    "        print(f\"  Bounding Box: {eq['bbox']}\")\n",
    "        print(f\"  Reconstructed Text: {eq_text}\")\n",
    "        print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    # I have added a heading to the sample data to test the new filter.\n",
    "import os\n",
    "# for filename in os.listdir(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\")):\n",
    "# with open(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\", filename), 'r') as f:\n",
    "#     sample_page_data = json.load(f)\n",
    "\n",
    "with open(os.path.join(\"data\", \"raw\", \"solvency-II-files\", \"solvency II - level 2.pdf\"), 'rb') as f:\n",
    "    original_doc = pymupdf.open(f)\n",
    "\n",
    "# page_number = 70\n",
    "\n",
    "# equations = detect_equations(original_doc[page_number - 1].get_text(\"dict\", flags=0), verbose=1)\n",
    "\n",
    "# print(f\"File: {filename}\")\n",
    "# print_equations(equations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "14a755e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation_spans = \"*Lapse*\" \"*up* ¼ 0,5 � *l* *up* � *n* *up* � *S* *up*\n",
    "equation_spans = [\"RM\", \"re,all\", \"�\", \"Recoverables\", \"[Recoverables]\", \"all\", \"[i]\"]\n",
    "test = {\"spans\": []}\n",
    "for equation_span in equation_spans:\n",
    "    test[\"spans\"].append({\"text\": equation_span, \"font\": 'Italic'})\n",
    "\n",
    "# is_likely_heading_or_prose(test, False)\n",
    "# calculate_positive_line_score(test, 12, 5)\n",
    "# 'ð'.isascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 equations.\n",
      "\n",
      "['CorrEQ', 'ð', 'r,s', 'Þ', ' �', 'SCR', 'ð', 'earthquake,r', 'Þ', ' �', 'SCR', 'ð', 'earthquake,s', 'Þ', 'Þ þ', ' SCR', '2', 'SCR', 'earthquake', ' ¼', 'ð', 'earthquake,other', 'Þ', 'ð', 'r,s', 'Þ']\n",
      "--- Equation 1 ---\n",
      "  Bounding Box: (82.716064453125, 240.76597595214844, 372.42034912109375, 259.61907958984375)\n",
      "  Reconstructed Text: CorrEQðr,sÞ �SCRðearthquake,rÞ �SCRðearthquake,sÞÞ þ SCR2SCRearthquake ¼ðearthquake,otherÞðr,sÞ\n",
      "--------------------\n",
      "\n",
      "['L', 'ð', 'earthquake,r', 'Þ', ' ¼', ' Q', 'ð', 'earthquake,r', 'Þ', ' �', 'Corr', 'ð', 'earthquake,r,i,j', 'Þ', ' �', 'WSI', 'ð', 'earthquake,r,i', 'Þ', ' �', 'WSI', 'ð', 'earthquake,r,j', 'Þ', 'ð', 'i,j', 'Þ']\n",
      "--- Equation 2 ---\n",
      "  Bounding Box: (82.7159423828125, 418.3394470214844, 359.936279296875, 436.727783203125)\n",
      "  Reconstructed Text: Lðearthquake,rÞ ¼ Qðearthquake,rÞ �Corrðearthquake,r,i,jÞ �WSIðearthquake,r,iÞ �WSIðearthquake,r,jÞði,jÞ\n",
      "--------------------\n",
      "\n",
      "['WSI', 'ð', 'earthquake,r,i', 'Þ', ' ¼', ' W', 'ð', 'earthquake,r,i', 'Þ', ' �', 'SI', 'ð', 'earthquake,r,i', 'Þ']\n",
      "--- Equation 3 ---\n",
      "  Bounding Box: (82.71573638916016, 588.3056640625, 231.7544403076172, 598.647216796875)\n",
      "  Reconstructed Text: WSIðearthquake,r,iÞ ¼ Wðearthquake,r,iÞ �SIðearthquake,r,iÞ\n",
      "--------------------\n",
      "\n",
      "['SI', 'ð', 'earthquake,r,i', 'Þ', ' ¼', ' SI', 'ð', 'property,r,i', 'Þ', ' þ', ' SI', 'ð', 'onshore-property,r,i', 'Þ']\n",
      "--- Equation 4 ---\n",
      "  Bounding Box: (82.71598815917969, 689.275146484375, 233.70298767089844, 699.6738891601562)\n",
      "  Reconstructed Text: SIðearthquake,r,iÞ ¼ SIðproperty,r,iÞ þ SIðonshore-property,r,iÞ\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "# 1. how to ensure that replacement is correct?\n",
    "# 2. how to replace the exact text?\n",
    "\n",
    "equations_detected = None\n",
    "with open(os.path.join(\"data\", \"raw\", \"test-data\", \"solvency II - level 2 - 78 - replacement-test.pdf\"), 'rb') as f:\n",
    "    doc = pymupdf.open(f)\n",
    "    for page_md in doc:\n",
    "        blocks = page_md.get_text(\"dict\", flags=0)[\"blocks\"]\n",
    "        equations = detect_equations({\"blocks\": blocks})\n",
    "        # tables = page.find_tables()\n",
    "        # pprint.pprint(tables)\n",
    "        print_equations(equations)\n",
    "        # page.get_pixmap(dpi=1200).save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))\n",
    "        equations_detected = equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820ae78",
   "metadata": {},
   "source": [
    "# Research on regex patterns to find equation on page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "30d3c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_1 = \"\"\"\n",
    "1. The capital requirement for earthquake risk shall be equal to the following:\n",
    "\n",
    "\n",
    "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n",
    "*SCR* *earthquake* ¼ ðX *CorrEQ* ð *r,s* Þ � *SCR* ð *earthquake,r* Þ � *SCR* ð *earthquake,s* Þ Þ þ *SCR* [2] ð *earthquake,other* Þ\n",
    "\n",
    "\n",
    "s\n",
    "\n",
    "\n",
    "*CorrEQ* ð *r,s* Þ � *SCR* ð *earthquake,r* Þ � *SCR* ð *earthquake,s* Þ Þ þ *SCR* [2] ð *earthquake,other* Þ\n",
    "\n",
    "\n",
    "ð *r,s* Þ\n",
    "\n",
    "\n",
    "where:\n",
    "\"\"\"\n",
    "snippet_2 = \"\"\"\n",
    "following amount:\n",
    "\n",
    "\n",
    "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n",
    "*L* ð *earthquake,r* Þ ¼ *Q* ð *earthquake,r* Þ � X *Corr* ð *earthquake,r,i,j* Þ � *WSI* ð *earthquake,r,i* Þ � *WSI* ð *earthquake,r,j* Þ\n",
    "\n",
    "\n",
    "s\n",
    "\n",
    "\n",
    "*Corr* ð *earthquake,r,i,j* Þ � *WSI* ð *earthquake,r,i* Þ � *WSI* ð *earthquake,r,j* Þ\n",
    "\n",
    "\n",
    "ð *i,j* Þ\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\", \n",
    "        api_key=os.environ[\"GOOGLE_API_KEY\"], \n",
    "        temperature=0.2,\n",
    ")\n",
    "\n",
    "page_number = 0\n",
    "test_image = \"data/raw/test-data/solvency_II_equations_page_{}.png\".format(page_number)\n",
    "\n",
    "def extract_equations(llm, image_url):\n",
    "        # from docs: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding\n",
    "        # * If you want to detect text in an image, use prompts with a single image to produce better results than prompts with multiple images.\n",
    "        # * If your prompt contains a single image, place the image before the text prompt in your request.\n",
    "\n",
    "        # from docs, may not be fully applicable but related:\n",
    "        # For most accurate OCR results from Document AI, document scans should be a minimum of 200 dpi (dots per inch). 300 dpi and higher generally produce the best results. OCR accuracy is dependent on both the resolution and the minimum font size, along with other factors like document (and if handwritten, handwriting) quality, so testing is recommended. The image quality analysis feature can help evaluate resolution concerns.\n",
    "        # we have precompute images where equation are detected for solvency level 2 at 400 dpi\n",
    "        with open(image_url, \"rb\") as f:\n",
    "                image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "                extraction_prompt = \"Extract equation(s) in latex from this page, only consider full equation, not inline reference to variable\"\n",
    "                format_examples = r\"\"\"\n",
    "                \\n# Example output format\n",
    "                Here are the equations from the page, in LaTeX format:\n",
    "```latex\n",
    "\\text{SCR}_{\\text{earthquake}} = \\sqrt{\\left(\\sum_{(r,s)} \\text{CorrEQ}_{(r,s)} \\cdot \\text{SCR}_{(\\text{earthquake},r)} \\cdot \\text{SCR}_{(\\text{earthquake},s)}\\right) + \\text{SCR}^2_{(\\text{earthquake,other})}}\n",
    "```\n",
    "```latex\n",
    "\\text{WSI}_{(\\text{earthquake},r,i)} = W_{(\\text{earthquake},r,i)} \\cdot \\text{SI}_{(\\text{earthquake}, r,i)}\n",
    "```\n",
    "```latex\n",
    "\\text{SI}_{(\\text{earthquake},r,i)} = \\text{SI}_{(\\text{property},r,i)} + \\text{SI}_{(\\text{onshore-property},r,i)}\n",
    "```\n",
    "\n",
    "                # Answer based on the provided image adhering to the example format\n",
    "                \"\"\"\n",
    "                message = HumanMessage(\n",
    "                content=[\n",
    "                        {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image_data}\"},\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": extraction_prompt + format_examples},\n",
    "                ]\n",
    "                )\n",
    "                # print(\"LLM INSTRUCTION\")\n",
    "                # pprint.pprint(message)\n",
    "                response = llm.invoke([message])\n",
    "\n",
    "        equations_converted = re.findall(r\"```latex(.*?)```\", response.text(), re.DOTALL)\n",
    "\n",
    "        return equations_converted\n",
    "\n",
    "# extract_equations(llm, test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d13d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total equations detected: 189\n",
      "Total not enough snippets: 48/189\n",
      "Total equations matched: 141/189\n",
      "Total matches accepted: 126/141\n",
      "Total matches not accepted: 8/141\n",
      "Total matches with duplicate match: 7/141\n",
      "Unresolved 15 of 74 pages: [69, 87, 111, 116, 127, 128, 274, 275, 276, 277, 278, 279, 280, 281, 285]\n",
      "Unresolved matches: [(116, [{'snippet': {'index': 0, 'score': 24, 'text': 'following:\\n\\n*stress* *i* ¼ minð *b* *i* � *dur* *i* ;1Þ\\n\\nwhere:\\n\\n'}, 'equation': {'index': 0, 'text': ['stress', 'i', ' ¼', ' min', 'ð', 'b', 'i', ' �', 'dur', 'i', ';1', 'Þ']}, 'status': 'ACCEPTED'}, {'snippet': {'index': 0, 'score': 24, 'text': 'following:\\n\\n*stress* *i* ¼ minð *b* *i* � *dur* *i* ;1Þ\\n\\nwhere:\\n\\n'}, 'equation': {'index': 1, 'text': ['stress', 'i', ' ¼', ' min', 'ð', 'b', 'i', ' �', 'dur', 'i', ';1', 'Þ']}, 'status': 'ACCEPTED'}, {'snippet': {'index': 0, 'score': 24, 'text': 'following:\\n\\n*stress* *i* ¼ minð *b* *i* � *dur* *i* ;1Þ\\n\\nwhere:\\n\\n'}, 'equation': {'index': 2, 'text': ['stress', 'i', ' ¼', ' min', 'ð', 'b', 'i', ' �', 'dur', 'i', ';1', 'Þ']}, 'status': 'ACCEPTED'}]), (127, [{'snippet': {'index': 0, 'score': 25, 'text': 'following:\\n\\n*LGD* ¼ max 50� % �ð *RE* cov *erables* þ 50% � *RM* *re* Þ − *F* � *Collateral* ; 0�\\n\\nwhere:\\n\\n'}, 'equation': {'index': 0, 'text': ['�', '�', 'LGD', ' ¼', ' max 50', '%', ' �ð', 'RE', 'cov', 'erables', ' þ', ' 50%', ' �', 'RM', 're', 'Þ', ' − ', 'F', ' �', 'Collateral', '; 0']}, 'status': 'ACCEPTED'}, {'snippet': {'index': 0, 'score': 25, 'text': 'following:\\n\\n*LGD* ¼ max 50� % �ð *RE* cov *erables* þ 50% � *RM* *re* Þ − *F* � *Collateral* ; 0�\\n\\nwhere:\\n\\n'}, 'equation': {'index': 1, 'text': ['�', '�', 'LGD', ' ¼', ' max 90', '%', ' �ð', 'RE', 'cov', 'erables', ' þ', ' 50%', ' �', 'RM', 're', 'Þ', ' − ', 'F', ' �', 'Collateral', '; 0']}, 'status': 'ACCEPTED'}, {'snippet': {'index': 2, 'score': 34, 'text': 'following:\\n\\n*LGD* ¼ maxð90%ð *Derivative* þ *RM* *fin* Þ − *F′* � *Collateral* ; 0Þ\\n\\nwhere\\n\\n'}, 'equation': {'index': 2, 'text': ['LGD', ' ¼', ' max', 'ð', '90', '%', 'ð', 'Derivative', ' þ', ' RM', 'fin', 'Þ', ' − ', 'F′', ' �', 'Collateral', '; 0', 'Þ']}, 'status': 'ACCEPTED'}, {'snippet': {'index': 3, 'score': 15, 'text': 'following:\\n\\n*LGD* ¼ maxð *Loan* − 80% � *Mortgage* ; 0Þ\\n\\nwhere:\\n\\n'}, 'equation': {'index': 3, 'text': ['LGD', ' ¼', ' max', 'ð', 'Loan', ' − 80%', ' �', 'Mortgage', '; 0', 'Þ']}, 'status': 'ACCEPTED'}])] \n",
      "quick fix 0\n"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "regex_without_page = r\"((following|follows)([^\\n]*?:|\\n))((?!following|follows).)*?(\\n((where(((?!where).)*?(?=\\(a\\))))|where|provided))\"\n",
    "# example of page within (following|follows) ... where block --- Page 78 --- in solvency II - level 2\n",
    "# regex_with_page = r'((following|follows)[^\\n]*?:)((?!following|follows).)*?(--- Page \\d+ ---)((?!following|follows).)*?(\\n(where|provided))'\n",
    "\n",
    "def find_equation_snippets(markdown_file):\n",
    "    matches = re.finditer(regex_without_page, markdown_file, re.DOTALL)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "def find_page(markdown_file, page_number, is_last_page):\n",
    "    page_regex = r\"(--- Page \" + str(page_number) + \" ---.*?)\"\n",
    "    if is_last_page:\n",
    "        match = re.search(page_regex, markdown_file, re.DOTALL)\n",
    "        return match.group(0)\n",
    "    \n",
    "    \n",
    "    page_regex += r\"(--- Page \" + str(page_number+1) + \" ---)(\\nwhere)?\"\n",
    "    match = re.search(page_regex, markdown_file, re.DOTALL)\n",
    "    if not match:\n",
    "        raise Exception(f\"Expected exactly one match for page {page_number}, found 0.\")\n",
    "    return match.group(1) + (match.group(3) + \"\\n\" if match.group(3) else \"\")\n",
    "\n",
    "def evaluate_placement_likelihood(ordered_equation_spans, snippets, verbose=0, std_output_file=None):\n",
    "    count_per_snippet = [0] * len(snippets)\n",
    "    count_in_expected_order = [0] * len(snippets)\n",
    "    for i, snippet in enumerate(snippets):\n",
    "        # snippet = snippet.replace(\" \", \"\")\n",
    "        # snippet = snippet.replace(\"*\", \"\")\n",
    "        snippet = snippet.replace(\"*\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \" \")\n",
    "        snippet_chunks = snippet.split(\" \")\n",
    "        clean_snippet_chunks = []\n",
    "        # remove empty chunks\n",
    "        for snippet_chunk in snippet_chunks:\n",
    "            if snippet_chunk != \"\":\n",
    "                clean_snippet_chunks.append(snippet_chunk)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ordered equation_spans: {ordered_equation_spans}\", file=std_output_file)\n",
    "            print(f\"evaluation for snippet {i}: {clean_snippet_chunks}\", file=std_output_file)\n",
    "        current_search_index = 0 # tries to take the order into account\n",
    "        current_window_extra_size = 0\n",
    "\n",
    "\n",
    "        for idx, span in enumerate(ordered_equation_spans):\n",
    "            span = span.replace(\" \", \"\")\n",
    "            # set up found_pos and search_window\n",
    "            snippet_window = []\n",
    "            if idx == 0 or current_search_index == 0: # find first snippet with match\n",
    "                # found_pos = snippet.find(span)\n",
    "                try: \n",
    "                    found_pos = clean_snippet_chunks.index(span)\n",
    "                except ValueError:\n",
    "                    found_pos = -1\n",
    "            else:\n",
    "                # SEARCH_WINDOW = len(span) * 3 if len(span) * 3 < len(snippet) else len(span)\n",
    "                # end = current_search_index + SEARCH_WINDOW + 1 + current_window_extra_size\n",
    "                # end = min(end, len(snippet))  # Ensure we don't go out of bounds\n",
    "                # found_pos = snippet.find(span, current_search_index, end)\n",
    "                SEARCH_WINDOW = current_window_extra_size + current_window_extra_size + 3 if current_search_index + current_window_extra_size + 3 < len(clean_snippet_chunks) else len(clean_snippet_chunks)\n",
    "                snippet_window = clean_snippet_chunks[current_search_index:current_search_index + SEARCH_WINDOW]\n",
    "                try:\n",
    "                    found_pos = current_search_index + snippet_window.index(span)\n",
    "                except ValueError:\n",
    "                    found_pos = -1\n",
    "            if verbose:\n",
    "                print(f\"WINDOW {snippet_window} for {span}\", file=std_output_file)\n",
    "                if found_pos != -1:\n",
    "                    print(f\"Found '{span}' at position {found_pos} in snippet {i}.\", file=std_output_file)\n",
    "\n",
    "            # update score accordingly\n",
    "            if span in clean_snippet_chunks:\n",
    "                # print(f\"Found '{span}' in snippet {i}.\")\n",
    "                count_per_snippet[i] += 1\n",
    "                if found_pos != -1:\n",
    "                    count_in_expected_order[i] += 1\n",
    "                    current_search_index = found_pos + 1\n",
    "                    current_window_extra_size = 0\n",
    "                else:\n",
    "                    current_window_extra_size += 1\n",
    "    return count_per_snippet, count_in_expected_order\n",
    "\n",
    "\n",
    "def convert_equations(best_matches, image_url):\n",
    "    equations_converted = extract_equations(llm, image_url)\n",
    "\n",
    "    if len(equations_converted) != len(best_matches):\n",
    "        raise Exception(\"[Error in converting equations to latex]\")\n",
    "    \n",
    "    for i, equation_converted in enumerate(equations_converted):\n",
    "        first_match = next((best_match for best_match in best_matches if best_match[\"snippet\"][\"index\"] == i), None)\n",
    "        # print(f\"equation_converted: {equation_converted}\")\n",
    "        first_match[\"equation\"][\"latex\"] = equation_converted\n",
    "\n",
    "\n",
    "def handle_replacement(best_matches, page_md, page_number):\n",
    "    # print(f\"\\nHandling replacement for page {page_number} with {len(best_matches)} best matches.\")\n",
    "    i = 0\n",
    "    number_replaced = 0\n",
    "\n",
    "    def replacer(match: re.Match):\n",
    "        # print(\"Found {} matches\".format(len(matches)))\n",
    "\n",
    "        nonlocal i\n",
    "        nonlocal best_matches\n",
    "        nonlocal number_replaced\n",
    "        # is the snippet a best_match?\n",
    "        first_match = next((best_match for best_match in best_matches if best_match[\"snippet\"][\"index\"] == i), None)\n",
    "        i += 1\n",
    "\n",
    "        equation_latex_llm = first_match[\"equation\"][\"latex\"] if first_match and \"latex\" in first_match[\"equation\"] else \"\"\n",
    "\n",
    "        if first_match:\n",
    "            latex_equation = f\"equation {equation_latex_llm} in snippet {first_match['snippet']['text']} replaced equation {first_match['equation']['text']}\"\n",
    "            number_replaced += 1\n",
    "\n",
    "            # if not match.group(11):\n",
    "                # match without page\n",
    "            # print(\"Match without page: \", match.group(2), match.group(3))\n",
    "            if match.group(7):\n",
    "                return f\"{match.group(1)}\\n<REPLACEMENT_EQUATION>\\n {latex_equation} \\n</REPLACEMENT_EQUATION>\\nwhere:\\n(a)\"\n",
    "            return f\"{match.group(1)}\\n<REPLACEMENT_EQUATION>\\n {latex_equation} \\n</REPLACEMENT_EQUATION>\\n{match.group(4)}\\n\"\n",
    "        else:\n",
    "            return match.group(0)\n",
    "\n",
    "        \n",
    "        # # replace equation\n",
    "\n",
    "    # regex = \"|\".join([regex_without_page, regex_with_page])\n",
    "\n",
    "    # page = response.text()\n",
    "    # with open(os.path.join(\"data\", \"preprocessed-step-final\", \"solvency-II-files\", \"final_solvency II - level 2.pdf.md\"), 'r', encoding=\"utf-8\") as f:\n",
    "    #     page = f.read()\n",
    "    page_delimiter = \"--- Page \" + str(page_number) + \" ---\"\n",
    "\n",
    "    page_delimiter_before = re.findall(page_delimiter, page_md)\n",
    "\n",
    "    (processed_text, equation_replaced) = re.subn(regex_without_page, replacer, page_md, flags=re.DOTALL)\n",
    "    # print(f\"Replaced {equation_replaced} equations on page.\")\n",
    "\n",
    "    # verify replacement did not change number of page delimiters\n",
    "    page_delimiter_after = re.findall(page_delimiter, processed_text)\n",
    "    if page_delimiter_after != page_delimiter_before:\n",
    "        print(f\"[ERROR, PAGE DELIMITER CHANGED]: Before: {page_delimiter_before}, After: {page_delimiter_after}\", file=std_output_file)\n",
    "        raise Exception(f\"Page delimiter on page: {page_number}\")\n",
    "\n",
    "    if number_replaced != len(best_matches):\n",
    "        print(f\"Expected {len(best_matches)} replacements, but got {number_replaced}.\")\n",
    "        raise Exception(f\"[ERROR, REPLACEMENT NOT SUCCESSFULL] for {page_number}\")\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def match_equations(original_doc, markdown_file, process_page_mask, file_name, std_output_file):\n",
    "    # markdown_file = pymupdf4llm.to_markdown(doc)\n",
    "    markdown_file_with_equation_replaced = \"\"\n",
    "    page_numbers_unresolved = []\n",
    "    equation_matching_unresolved = []\n",
    "    total_number_of_pages_with_equations = 0\n",
    "    total_matches_not_accepted = 0\n",
    "    total_equation_detected = 0\n",
    "    total_equation_matched = 0\n",
    "    total_equation_matched_accepted = 0\n",
    "    total_equation_matched_with_duplicate_match = 0\n",
    "    total_not_enough_snippets = 0\n",
    "\n",
    "    last_page_index = 0\n",
    "    COUNT_ERROR_ACCEPTANCE = 0.8 #the portion of spans that must be matched\n",
    "    ORDER_ERROR_ACCEPTANCE = 0.25\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(process_page_mask) - 1, -1, -1):\n",
    "        if process_page_mask[i]:\n",
    "            last_page_index = i\n",
    "            break\n",
    "\n",
    "    for page in original_doc: #[original_doc[36], original_doc[42]]:\n",
    "        if process_page_mask[page.number] == 0:\n",
    "            # print(f\"Skipping page {page.number + 1} (masked)\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"page with {page.number + 1} is being processed\")\n",
    "        page_text_in_dict = page.get_text(\"dict\", flags=0)\n",
    "        # detect_equation will look if the page has equations\n",
    "        equations_detected = detect_equations(page_text_in_dict)\n",
    "        total_equation_detected += len(equations_detected)\n",
    "        # find_snippets will try to identify the exact replacement area\n",
    "        # based on following: where regex\n",
    "\n",
    "        page_md = find_page(markdown_file, page.number + 1, last_page_index == page.number)\n",
    "        if equations_detected:\n",
    "            print(f\"*** Found {len(equations_detected)} equations on page {page.number + 1}. ***\", file=std_output_file)\n",
    "            total_number_of_pages_with_equations += 1\n",
    "\n",
    "            # the original doc will not be changed, the snippets correspond the current representation of the original document in markdown.\n",
    "            snippets = find_equation_snippets(page_md)\n",
    "\n",
    "            \n",
    "            if len(snippets) < len(equations_detected):\n",
    "                print(f\"[ERROR, NOT ENOUGH SNIPPETS]: Found {len(snippets)}, expected {len(equations_detected)}.\", file=std_output_file)\n",
    "                if len(snippets) != 0:\n",
    "                    # not implemented yet\n",
    "                    equation_texts = []\n",
    "\n",
    "                    for idx, eq in enumerate(equations_detected):\n",
    "                        equation_text = [s['text'] for s in eq['spans']]\n",
    "                        print(f\"Equations text: {equation_text}\", file=std_output_file)\n",
    "                        equation_texts.append(str(equation_text))\n",
    "                        # print(f\"Equations obj: {eq}\", file=std_output_file)\n",
    "                    for snippet in snippets:\n",
    "                        print(f\"Snippet text: {snippet}\", file=std_output_file)\n",
    "                    # raise Exception(f\"Not enough snippets found. Found {len(snippets)}, expected {len(equations_detected)}.\")\n",
    "                    page_numbers_unresolved.append(page.number + 1)\n",
    "                total_not_enough_snippets += len(equations_detected)\n",
    "                markdown_file_with_equation_replaced += page_md #+ f\"<ERROR, NOT ENOUGH SNIPPETS>\\nsnippets:\\n{snippets} \\n\\n equations:\\n{\"\\n\".join([str(idx + 1) + \".\" + equation_text for idx, equation_text in enumerate(equation_texts)])}\\n</ERROR, NOT ENOUGH SNIPPETS>\\n\"\n",
    "                continue\n",
    "                \n",
    "            best_matches = []\n",
    "\n",
    "\n",
    "            for idx, eq in enumerate(equations_detected):\n",
    "                equation_spans = [s['text'] for s in eq['spans']]\n",
    "                # print(f\"\\n*** Evaluating spans from equation {idx + 1}: {spans} ***\")\n",
    "\n",
    "                count_per_snippet, count_in_expected_order = evaluate_placement_likelihood(equation_spans, snippets)\n",
    "                # print(f\"Count per snippet: {count_per_snippet}\")\n",
    "                # max_index, max_count = max(enumerate(count_per_snippet), key=lambda x: x[1])\n",
    "                # max_index_in_expected_order, max_in_expected_order = max(enumerate(count_in_expected_order), key=lambda x: x[1])\n",
    "\n",
    "                highest_score = 0\n",
    "                best_match = {\"snippet\":{\"index\": None, \"score\": 0, \"text\": None}, \"equation\": {\"index\": idx, \"text\": equation_spans}}\n",
    "                snippet_scores = []\n",
    "                for i, snippet in enumerate(snippets):\n",
    "                    snippet_score = count_per_snippet[i] + 2 * count_in_expected_order[i]\n",
    "                    snippet_scores.append(snippet_score)\n",
    "                    if snippet_score > highest_score:\n",
    "                        highest_score = snippet_score\n",
    "                        best_match.update({\"snippet\": {\"index\": i, \"score\": snippet_score, \"text\": snippet}})\n",
    "                    # elif snippet_score == highest_score:\n",
    "                        # raise Exception(\"A tie, multiple snippets have the same score.\")\n",
    "\n",
    "                SCORE_THRESHOLD = len(equation_spans) * ORDER_ERROR_ACCEPTANCE + len(equation_spans) * COUNT_ERROR_ACCEPTANCE\n",
    "                if SCORE_THRESHOLD > highest_score:\n",
    "                    print(f\"<Equation {idx + 1} on page {page.number + 1} did not match sufficiently with any snippet.>\", file=std_output_file)\n",
    "                    # print(f\"Did not meet total_score requirement, potentially {count_per_snippet[best_match_index]} / {len(equation_spans) * COUNT_ERROR_ACCEPTANCE} in snippet {best_match_index + 1}.\", file=std_output_file)\n",
    "                    pprint.pprint(count_per_snippet, stream=std_output_file)\n",
    "                    # print(f\"Did not meet total_score requirement, potentially {count_in_expected_order[best_match_index]} / {len(equation_spans) * ORDER_ERROR_ACCEPTANCE} in snippet {best_match_index + 1}.\", file=std_output_file)\n",
    "                    pprint.pprint(count_in_expected_order, stream=std_output_file)\n",
    "                    # raise Exception(\"Not accepted match\")\n",
    "                    evaluate_placement_likelihood(equation_spans, snippets, verbose=1, std_output_file=std_output_file)\n",
    "                    print(f\"<End of equation NOT_ACCEPTED >\", file=std_output_file)\n",
    "                    # total_matches_not_accepted += 1\n",
    "                    best_match[\"status\"] = \"NOT_ACCEPTED\"\n",
    "                else:\n",
    "                    best_match[\"status\"] = \"ACCEPTED\"\n",
    "                \n",
    "                    \n",
    "                best_matches.append(best_match)\n",
    "                # print(f\"SNIPPET SCORES: {snippet_scores}\")\n",
    "                # print(f\"BEST MATCH FOR EQUATION IS: {best_match_index + 1 if isinstance(best_match_index, int) else best_match_index} with score {highest_score}.\")\n",
    "                # print(f\"*** END OF Evaluating spans from equation {idx + 1} ***\\n\")\n",
    "\n",
    "            total_equation_matched += len(best_matches)\n",
    "\n",
    "            snippets_indexes = [best_match[\"snippet\"][\"index\"] for best_match in best_matches if best_match[\"status\"] == \"ACCEPTED\"]\n",
    "            if \"NOT_ACCEPTED\" in [best_match[\"status\"] for best_match in best_matches]:\n",
    "                page_numbers_unresolved.append(page.number + 1)\n",
    "                total_matches_not_accepted += len(best_matches)\n",
    "                print(f\"[ERROR, MATCHES NOT ACCEPTED] SKIPPING PAGE {page.number + 1}...\", file=std_output_file)\n",
    "                markdown_file_with_equation_replaced += page_md\n",
    "                continue\n",
    "            elif len(set(snippets_indexes)) != len(snippets_indexes):\n",
    "                print(\"[ERROR, MATCHED MULTIPLE TIMES] \", file=std_output_file)\n",
    "\n",
    "                for best_match in best_matches:\n",
    "                    print(f\"Best match for equation {best_match['equation']['text']} on page {page.number + 1}:\\n{best_match['snippet']['text']}\\n\\n\", file=std_output_file)\n",
    "\n",
    "                # for eq in equations_detected:\n",
    "                #     equation_text = [s['text'] for s in eq['spans']]\n",
    "                #     print(f\"Equations text: {equation_text}\", file=std_output_file)\n",
    "                # for idx, snippet in enumerate(snippets):\n",
    "                #     print(f\"*** MATCH FOR SNIPPET {idx + 1} ***\", file=std_output_file)\n",
    "                #     print(f\"Snippet {idx + 1}:\\n{snippet}\", file=std_output_file)\n",
    "                #     print(f\"Best match for snippet {idx + 1}:\\n{[s['text'] for s in equations_detected[best_matches[idx]]['spans']] if idx < len(best_matches_indexes) else 'None'}\\n\", file=std_output_file)\n",
    "                #     print(f\"*** END OF BEST MATCH OF SNIPPET {idx + 1} ***\", file=std_output_file)\n",
    "\n",
    "                print(f\"Best matches: {best_matches}\", file=std_output_file)\n",
    "                equation_matching_unresolved.append((page.number + 1, best_matches))\n",
    "                page_numbers_unresolved.append(page.number + 1)\n",
    "                total_equation_matched_with_duplicate_match += len(best_matches)\n",
    "                markdown_file_with_equation_replaced += page_md\n",
    "                continue\n",
    "                # raise Exception(\"equation matched with multiple\")\n",
    "\n",
    "            image_url = f\"data/raw/test-data/page-images/{file_name}/page_{page.number + 1}-{dpi}.png\"\n",
    "            # convert_equations(best_matches, image_url) # adds information to best_matches\n",
    "            page_md_with_latex_equations = handle_replacement(best_matches, page_md, page.number + 1)\n",
    "            markdown_file_with_equation_replaced += page_md_with_latex_equations\n",
    "            total_equation_matched_accepted += len(best_matches)\n",
    "            # print(f\"Equation text: {''.join([s['text'] for s in eq['spans']])}\")\n",
    "        # tables = page.find_tables()\n",
    "        # pprint.pprint(tables)\n",
    "        # print_equations(equations_detected)\n",
    "        # page.get_pixmap(dpi=1200).save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))\n",
    "            print(f\"[ACCEPTED] page was matched\", file=std_output_file)\n",
    "            print(f\"*** End of page {page.number + 1}. ***\", file=std_output_file)\n",
    "        else:\n",
    "            print(f\"[NO EQUATIONS] page {page.number + 1} has no equations.\", file=std_output_file)\n",
    "            markdown_file_with_equation_replaced += page_md\n",
    "            # continue\n",
    "        \n",
    "\n",
    "    print(f\"Total equations detected: {total_equation_detected}\",)\n",
    "    print(f\"Total not enough snippets: {total_not_enough_snippets}/{total_equation_detected}\")\n",
    "    print(f\"Total equations matched: {total_equation_matched}/{total_equation_detected}\")\n",
    "    print(f\"Total matches accepted: {total_equation_matched_accepted}/{total_equation_matched}\")\n",
    "    print(f\"Total matches not accepted: {total_matches_not_accepted}/{total_equation_matched}\")\n",
    "    print(f\"Total matches with duplicate match: {total_equation_matched_with_duplicate_match}/{total_equation_matched}\")\n",
    "    print(f\"Unresolved {len(page_numbers_unresolved)} of {total_number_of_pages_with_equations} pages: {page_numbers_unresolved}\")\n",
    "    print(f\"Unresolved matches: {equation_matching_unresolved} \")\n",
    "\n",
    "    return markdown_file_with_equation_replaced\n",
    "    \n",
    "\n",
    "# test_file = \"solvency II - level 2 - 1-295.pdf\"\n",
    "\n",
    "\n",
    "# will not be updated. for testing only\n",
    "copy_structure_solvency_II_level_2 ={\n",
    "  \"file_name\": \"solvency II - level 2.pdf\",\n",
    "  \"toc\": (1, 4),\n",
    "  \"recitals\": (5, 20),\n",
    "  \"CORRELATION_TABLES\": (296, 797),\n",
    "  \"NUM_TITLES\": 3,\n",
    "  \"NUM_CHAPTERS\": 25,\n",
    "  \"NUM_SECTIONS\": 61,\n",
    "  \"NUM_SUBSECTIONS\": 31,\n",
    "  \"NUM_ARTICLES\": 381\n",
    "}\n",
    "\n",
    "file_name = \"solvency II - level 2\"\n",
    "precompute_images = False\n",
    "\n",
    "with open(\"print-output.txt\", \"w\", encoding=\"utf-8\") as std_output_file:\n",
    "\n",
    "    with open(os.path.join(\"data\", \"raw\", \"solvency-II-files\", f\"{file_name}.pdf\"), 'rb') as f:\n",
    "        original_doc = pymupdf.open(f) # detection on original pdf\n",
    "        # markdown_file = pymupdf4llm.to_markdown(original_doc) # can be changed to own preprocessed markdown_file\n",
    "        mask : list = [1] * original_doc.page_count\n",
    "\n",
    "        exceptions = [copy_structure_solvency_II_level_2[\"toc\"], copy_structure_solvency_II_level_2[\"CORRELATION_TABLES\"]]\n",
    "\n",
    "        for start, end in exceptions:\n",
    "            for i in range(start - 1, end): #page delimiters are one-indexed\n",
    "                if i >= original_doc.page_count:\n",
    "                    raise Exception(f\"Page index {i} out of bounds for document with {original_doc.page_count} pages.\")\n",
    "                mask[i] = 0\n",
    "\n",
    "        dpi = 400\n",
    "\n",
    "        if precompute_images:\n",
    "            # https://cloud.google.com/document-ai/docs/file-types\n",
    "            # For most accurate OCR results from Document AI, document scans should be a minimum of 200 dpi (dots per inch). 300 dpi and higher generally produce the best results. OCR accuracy is dependent on both the resolution and the minimum font size, along with other factors like document (and if handwritten, handwriting) quality, so testing is recommended. The image quality analysis feature can help evaluate resolution concerns.\n",
    "            start_time = time.time()\n",
    "            for page in original_doc:\n",
    "                page_text_in_dict = page.get_text(\"dict\", flags=0)\n",
    "                # detect_equation will look if the page has equations\n",
    "                equations_detected = detect_equations(page_text_in_dict)\n",
    "                if equations_detected:\n",
    "                    # print(f\"Converting page to image...\")\n",
    "                    file_path = f\"data/raw/test-data/page-images/{file_name}/page_{page.number + 1}-{dpi}.png\"\n",
    "                    if os.path.isfile(f\"{file_path}\"):\n",
    "                        # print(f\"Image already exists: {file_path}\")\n",
    "                        continue\n",
    "                    page_image = page.get_pixmap(dpi=dpi).save(file_path)\n",
    "            end_time = time.time()\n",
    "            print(f\"Converted images in {end_time - start_time} seconds.\")\n",
    "\n",
    "        with open(os.path.join(\"data\", \"preprocessed-step-2\", \"solvency-II-files\", \"substep-5-solvency II - level 2.pdf.md\"), 'r', encoding=\"utf-8\") as md_file:\n",
    "            markdown_file = md_file.read()\n",
    "\n",
    "        # markdown_file = pymupdf4llm.to_markdown(original_doc) # can be changed to own preprocessed markdown_file\n",
    "        markdown_file_with_equation_replaced = match_equations(original_doc, markdown_file, mask, file_name, std_output_file)\n",
    "\n",
    "        # quick fix, not sure why on same line: where--- Page \\d+\n",
    "        (markdown_file_with_equation_replaced, num_quick_fix) = re.subn(r\"(where)(--- Page \\d+)\", r\"\\1\\n\\2\", markdown_file_with_equation_replaced)\n",
    "        print(f\"quick fix {num_quick_fix}\")\n",
    "\n",
    "page_numbers = re.findall(r\"--- Page (\\d+) ---\", markdown_file_with_equation_replaced)\n",
    "\n",
    "page_tracker = 5\n",
    "if mask.count(1) != len(page_numbers):\n",
    "    for page_number in page_numbers:\n",
    "        if int(page_number) != page_tracker:\n",
    "            print(f\"Warning: Page number {page_number} does not match expected page number {page_tracker}.\")\n",
    "            page_tracker = int(page_number)\n",
    "        page_tracker += 1\n",
    "    print(f\"Warning: Number of pages in mask ({mask.count(1)}) does not match number of pages in markdown ({len(page_numbers)}).\")\n",
    "\n",
    "with open(os.path.join(\"data\", \"preprocessed-step-2\", \"solvency-II-files\", \"substep-6_solvency II - level 2.pdf.md\"), 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_file_with_equation_replaced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "768c7e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(556, 0, 1235, 165, 1, '', '', 'I0', 'CCITTFaxDecode', 0), (1721, 0, 39, 48, 1, '', '', 'I1', 'CCITTFaxDecode', 0), (1720, 0, 34, 64, 1, '', '', 'I2', 'CCITTFaxDecode', 0), (1719, 0, 31, 62, 1, '', '', 'I3', 'CCITTFaxDecode', 0), (1718, 0, 1155, 388, 1, '', '', 'I4', 'CCITTFaxDecode', 0), (1720, 0, 34, 64, 1, '', '', 'I5', 'CCITTFaxDecode', 0), (1719, 0, 31, 62, 1, '', '', 'I6', 'CCITTFaxDecode', 0), (557, 0, 1020, 267, 1, '', '', 'I7', 'CCITTFaxDecode', 0), (1720, 0, 34, 64, 1, '', '', 'I8', 'CCITTFaxDecode', 0), (1719, 0, 31, 62, 1, '', '', 'I9', 'CCITTFaxDecode', 0)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"data\", \"raw\", \"solvency-II-files\", f\"{file_name}.pdf\"), 'rb') as f:\n",
    "        original_doc = pymupdf.open(f) # detection on original pdf\n",
    "\n",
    "        for page in [original_doc[272]]:\n",
    "            for image in page.get_images(full=True):\n",
    "                 xref = image[0]\n",
    "                 image_data = original_doc.extract_image(xref)\n",
    "                 with open(f\"image-page-{page.number + 1}-{xref}.{image_data['ext']}\", \"wb\") as img_file:\n",
    "                    img_file.write(image_data[\"image\"])\n",
    "            print(page.get_images(full=True))\n",
    "            print(page.get_xobjects())\n",
    "\n",
    "            page_text = page.get_text()\n",
    "            with open(f\"text-page-{page.number + 1}.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(page_text)\n",
    "            # print(page.get_image_info(0)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00760903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_data = original_doc.extract_image(170)\n",
    "\n",
    "# with open(f\"image.{image_data['ext']}\", \"wb\") as img_file:\n",
    "#     img_file.write(image_data[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57ac1da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 485,\n",
       " 'height': 94,\n",
       " 'ext': 'png',\n",
       " 'colorspace': 1,\n",
       " 'xres': 96,\n",
       " 'yres': 96,\n",
       " 'bpc': 1,\n",
       " 'size': 1404,\n",
       " 'image': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\xe5\\x00\\x00\\x00^\\x08\\x00\\x00\\x00\\x008!S\\xbc\\x00\\x00\\x00\\tpHYs\\x00\\x00\\x0e\\xc4\\x00\\x00\\x0e\\xc4\\x01\\x95+\\x0e\\x1b\\x00\\x00\\x05.IDATx\\x9c\\xed\\x9a\\xdb\\x96\\xc3 \\x08E\\xf3\\xff?\\xed\\xac5m\\x13\\xe5&\\n\\x9a\\x189/3Q\\x04e\\x07c\\xda\\x1eGh\\xbe\\xd2\\xc2\\xdeCJ\\xa5\\xc1\\x94\\x03\\xf3\\xfdJ\\xc3)\\x8c\\x8f\\x10\\xaah\\x06\\x82\\xc0|\\xb3\\xe6\\x00\\x08\\xcc\\xb7jV\\xfa\\x03\\xf3\\x8d\\x9a\\x96\\xfc\\x14\\x98o\\xd3\\xc4\\xdc\\x07\\xe6\\xdb43\\xf3A\\xf9&M\\xad\\xaf(\\xe6{49\\xef\\x81\\xf9\\x16\\x05\\xe5\\r4=\\xeb\\x81\\xf9\\x06\\x05\\xe5\\r\\x94n\\xa1\\x1c\\x98\\xe7JNy\\xfa\\xea\\xbc\\x1e\\x162AyD\\n}$%\\x14g=\\xc3m`\\xc2\\x0e\\x08\\xc2\\x83$$5\\xeb\\xa2\\xd3\\xdf\\x0b\\x85\\x1f\\x13\\x8c\\x87HHk\\xd9!Pv\\x0c\\x1a\\x94\\x87H\\x0b\\x996\\x1dC\\xb9\\xd9_\\xa8\\xa2\\x96|3\\x94\\x1d\\xa3F)\\x0f\\x91\\\\U\\xb5\\xa6n(\\xdc\\xc0\\x80<D,\\'\\xb2\\xdd\\xab\\x94\\x83\\xf2\\\\\\x05\\xe5\\r\\xc4\\x1f\\x92gP&\\xfc\\x07\\xe5\\x01\\xb2Q\\xee\\x87\"Q\\xee\\xf1\\x17\\x92Ty\\xa5\\xc1\\xc7/lc\\x88\\x1b\\x94\\xe7\\xa8\\x952a\\xe3\\x18x\\x97\\r\\xfbZ$Z\\xb0\\xb8\\xb5z\\x9f\\x82\\x98.\\xaf\\r[\\xa2\\xdc\\xe7o!]\\xabD\\xe88\\x96)\\x17\\xd9\\n[\\x88\\xd1\\xfcd\\xc4\\xa4[\\xa0\\x90\\xde7\\xa0\\x9cS\\x80\\x90H\\x94\\x07\\xbe-ho\\x07i\\x91YU\\xe7#\\xccX\\xbd>E\\xe4\\xf7S\\xce)\\xc2\\x02M\\xb0!\\x1fC^Pi$R(\\x82\\xacb\\xf6\\xa6\\xfc~\\xc8\\xff\\xfa.\\xb3,\\xe8\\x0c\\x0fE\\xb9\\x1c,\\xd5.\\x91B\\x99c\\x05s\\xad\\xd4EmOYx\\x18\\x97y\\x90*\\xfb\\xa0\\xb0\\xd3\\x01\\xf9\\xcc\\xca\\x98MP\\xd6\\xa7\\x9cd\\xd5\\x06bG\\x8cA\\x852\\xb2fgZ]\\t\\xdf\\xc9/F\\x16\\xe1\\xb8\\x96\\x9d\\x87\\xc9FY\\xdf\\xd0DYz\\x0b\\xab/\\x85\\xed\\xe2\\x87\\xca\\xe2(\\xf7\\xfa\\x9b\\xaf\\xa7P\\xceAH\\x1f}T\\xdf\\x96H\\x13\\x1b\\x94\\xf5)w\\xab\\x9128O\\x11Y:S\\xd9\\xfdN\\xccc\\x0e\\xca\\x9dj\\xa5\\x8c\\x07s\\x9f\\x19\\xf6Sf1{S\\xde\\x05\\xb2\\x81rbXp\\xede\\xb7bV\\xf46!\\x0fT8\\xf5s\\xb8\\x8ez)\\x7fZ\\xf9\\x8a\\xab\\xbd\\x11k\\xa6\\x05\\xcd\\x82r\\xaf\\xfa(\\xff\\x9a\\x84;\\xc0Fy\\xc4\\xcb-r\\xa9\\x9b\\xc9\\x1b\\xd4C\\xf9j`\\xd2d\\xde\\xb1I\\x1fV(4\\xe5~\\x7f\\x0b\\xa9\\x9dr~)Rn9}in\\x15+\\x94\\xa0\\xaco(\\xae\\xa4\\x1d;(\\xbb+\\xc9\\xaa\\x0clhP\\xe4\\xe9\\xf3\\x1aU\\xc1\\x8c\\x87T\\xa7\\xe6C\\xb9\\xee\\xef\\xb9\\xe0gQ\\xd6$\\xea\\xfb\\x99\\x08\\x17\\xb7\\x97\\xb2\\xb9\\xf2\\x94\\x94\\x9f\\x0by*e\\xc9\\xf88\\x89\\xb1\\x81\\xa9)=\\x88\\xf2Z{\\xb8Rm\\x94a\\xa7P\\x96\\x8f\\xa3\\x0c\\xe32\\x13\\x0c\\xca\\n\\xca`sW\\x1e\\xc0Y\\xcap\\x98\\xb0\\x92\\x9aH\\xca\\x84\\x95!\\xc4c\\xe5LY\\xc3\\x05\\xb7\\xf3vB\\xa8V\\xa96\\xec\\xd6\\x18:{\\xe3\\xcc\\xed\\xea\\xa0,lz\\xf8\\x16`_\\xb4`Kmf\\xde\\x94\\xc9\\xc95\\xc7\\x90\\xed\\x93\\xcaj\\x82,\\x943\\x8c\\xe7\\xc3\\x18\\x8d\\xd4R\\xa6\\xca\\xcas\\xc3\\xd6Pn\\x0f!\\x0eHKS\\xfe\\x82M\\xe9L\\xd5y\\xae&\\x9cW\\xabT\\x95q\\'\\xca\\xc0\\x9ff\\xeb1\\x04\\xf4\\xf3eT\\x1b\\xe5\\xf2\\x8d-\\xbb\\xfcu\\x12\\xce\\xe9*\\xad%\\xd8\\x1f\\x01\\xbf\\x0e\\xb0(/=\\x872ZZ\\xad\\xa1\\xcc\\x07\\x84L\\x95(\\xce\\x1c\\x13\\x02FQxj^)X\\xc2@\\xc8\\xbew\\x8cAhy\\xf5\\x06\\x00\\x96\\xca\\x1d\\xeb\\x1d\\x04\\xce/\\x91=\\xde[\\xad\\x1c\\xfa\\xc6~\\x07\\xfdn\\xe9\\xb3\\xb1\\xe8\\xa5m\\x8b\\xcc0\\xf38\\xff\\x7f\\xca-q\\n\\xb1\\xect\\x91]\\x02\\xdf\\xdd>\\x0fa\\x9b\\xecq{\\x01+\\xb6\\xb4\\xcf?\\xe8f,\\x83\\x147\\x7fiu\\xc0.\\xd3\\xed\\xfb\\\\\\xb9/\\n\\xe6\\xd8%d\\x015\\xf7@T \\xb0\\xcd\\xfb\\x88\\xffJ\\xa3\\xef=\\xd38\\xbb\\xe7\\xcb\\xfb\\xdeE9\\xf6\\x88\\x08\\xf7\\xe5\\x94\\xe3\\xfa\\xf50\\xb6\\xa51\\x0b\\xfe\\xb3)\\x94}/\\x92/e\\xfa9i\\x0c(R\\x82\\xd7\\n\\xa2e\\xe5_h\\x89g\\xf7[\\xe4\\xba*bo\\xb4\\x07\\x94k\\x91\\xedh\\xa6\\xfc\\xd0\\xd3\\x97\\x8f\\x1c1SO@{8\\xe2\\x80ym\\xc7\\xa8\\x83\\xb2\\x05M\\xe5!\\xec:uUf\\xbe\\xb4\\\\)gN\\xbd\\xc21\\x87\\xa7\\xec\\x88l(\\xe5\\xf3\\xd4E<\\xc5\\xdf%\\xb7\\x85\\x8d*e\\x962\\xc2\\xa6?h\\x95m\\xd4\\x81\\xfcm\\x9aL\\xd9\\xe0\\x14\\xec\\x15<?\\n[P\\xf6X\\x1a\\xcd\\xc3\\x1aJ\\xa0\\x8c:\\x08\\xdb\\x82\\x1f\\xa2|\\xed\\xfc\\xf5w\\x83\\xe5\\xe5\\xb36%\\xe5^\\xaf\\x85\\xd3\\xebL\\x0cL\\xa0\\xad\\xfa\\x84\\xdd;\\xbf\\x85\\xe4K\\xd9\\x13\\xf2\\xb5\\xcf\\x16\\xf5\\x8b\\x90\\x1f\\x89\\xb6\\xbd\\x06!+f\\xa7?\\x9e\\xfc\\xdbQ\\x9b<0\\xd3<\\xacQ~\\x07\\xad\\xb2~\\x0bOg\\x07a{\\r\\xca\\xff\\x03NJ\\xca\\xaf\\x85\\xec\\x82\\x99\\xe6a\\x8e\\x91\\xd2y\\xa2\\xce\\xb7Z`\\xc0\\xd8^}\\x99\\xd5\\x81\\x1f\\xc6\\xd8\\xd5;\\xe5\\x84\\x19\\xf1\\xb0G \\xcf\\xc6\\x8ccdK?rK\\x9a\\x9a\\x03\\xc5k\\xe4\\x82\\xb9\\xf8\\xeb\\xec>\\xe4\\xa1\\xb1\\x18\\x02r(\\x14\\n\\x85B\\xa1\\xe5u\\xbe\\xb8\\xdc=\\x91\\xd00\\x9d\\xaf\\xa6\\x01\\xf9\\xbdB_\\xba\\x84\\xde\\xa7\\x1d\\xbe+\\xdb^{|W\\xb6\\xb96\\xf9Fts\\xe1\\xcf\\xe7C\\xafS\\x94\\xf2\\x0e\\n\\xca\\x1bh\\xab/D\\xb7UP\\xdeA\\x8a\\xdf\\xd1\\x85\\x96W\\xfdwt\\xa1\\xf5U\\xfd\\x1d]\\xe8\\r\\xaa\\xfc\\x8e.\\xf4\\x0e\\x89\\xbf\\xa3[E\\x7fQ\\t0\\xa3\\x97\\xda\\x90+\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82',\n",
       " 'smask': 0,\n",
       " 'cs-name': 'None'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717a151",
   "metadata": {},
   "source": [
    "# Integration into existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3408f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question on interference. I need access \n",
    "\n",
    "# Integration\n",
    "# instead of doc[0].get_text() you can just use the langchain.Document.page_content\n",
    "# but the problem is you need the doc[0].get_text(\"dict\", flags=0) representation as well\n",
    "# therefore preferably I get access to the pymupdf.Document in the pymupdf4llm code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f4a0f",
   "metadata": {},
   "source": [
    "# Exploring other solvency II-files\n",
    "The solvency II - level 2 regulation seems to be the most heavy on equations (~100). However, we will also analyse if other files have many equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49587516",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cefc3ac",
   "metadata": {},
   "source": [
    "with open(\"print-output.txt\", \"w\", encoding=\"utf-8\") as std_output_file:\n",
    "    with open(os.path.join(\"data\", \"raw\", \"test-data\", \"equation-examples\", test_file), 'rb') as f:\n",
    "        match_equations(f, std_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regulation-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
