{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a31b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF 1.26.3: Python bindings for the MuPDF 1.26.3 library (rebased implementation).\n",
      "Python 3.13 running on win32 (64-bit).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "\n",
    "print(pymupdf.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a5d82",
   "metadata": {},
   "source": [
    "# Problem\n",
    "* Based on https://github.com/pymupdf/PyMuPDF/discussions/763\n",
    "* Start with looking at PyMuPDF PDF engine and its capabilities\n",
    "* No build in equation detection, especially problematic for text based equation\n",
    "\n",
    "# Pipeline\n",
    "1. Equation detection\n",
    "    * (option) verify detection\n",
    "2. Equation conversion latex equation\n",
    "    * (option) verify conversion\n",
    "3. Replacing original equation with latex equation\n",
    "\n",
    "\n",
    "## Equation detection\n",
    "### 1. heuristic based equation detection\n",
    "In our case code examples are not a problem:\n",
    "\n",
    "Yes, exactly!\n",
    "In PDF, text is just text. The PDF specification contains nothing to sub-divide different kinds of text. Equations are also text and be coded in any font, can be italic, or normal, mono-spaced of proportional, serifed or sans-serifed.\n",
    "Also note that the equation symbol appears in program code listings a lot - PyMuPDF.pdf is full of such examples.\n",
    "\n",
    "So I would say, that you have to develop your own way of recognizing equations ... and whatever you will develop, may not work with the next PDF example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910be774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9add5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# test_folder = os.path.join(\"data\", \"raw\", \"test-data\", \"equation-examples\")\n",
    "\n",
    "# for file_name in os.listdir(test_folder):\n",
    "#     doc = pymupdf.open(os.path.join(test_folder, file_name))\n",
    "#     for page in doc:\n",
    "#         # print(page.get_text(\"html\"))\n",
    "#         text_in_dict = page.get_text(\"dict\", flags=0)\n",
    "#         # tables = page.find_tables()\n",
    "#         # pprint.pprint(blocks)\n",
    "#         with open(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\", f'{file_name}-page_{page.number}.json'), 'w') as f:\n",
    "#             json.dump(text_in_dict, f)\n",
    "#         # page.get_pixmap().save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea \n",
    "# is if you can detect the equation\n",
    "# then you can just image multimodal, somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "679555a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line: ['Article 105 '] with score -5\n",
      "Processing line: ['Simplified calculation for captive insurance or reinsurance undertakings of the capital requirement for spread '] with score -5\n",
      "Processing line: ['risk on bonds and loans '] with score -5\n",
      "Processing line: ['Where Articles 88 and 89 are complied with, captive insurance or captive reinsurance undertakings may base the '] with score -5\n",
      "Processing line: ['calculation of the capital requirement for spread risk to in Article 176 on the assumption that all assets are assigned to '] with score -5\n",
      "Processing line: ['credit quality step 3. '] with score -5\n",
      "Processing line: ['Article 106 '] with score -5\n",
      "Processing line: ['Simplified calculation of the capital requirement for market risk concentration for captive insurance or '] with score -5\n",
      "Processing line: ['reinsurance undertakings '] with score -5\n",
      "Processing line: ['Where Articles 88 and 89 are complied with, captive insurance or captive reinsurance undertakings may use all of the '] with score -5\n",
      "Processing line: ['following assumptions for the calculation of the capital requirement for concentration risk: '] with score -5\n",
      "Processing line: ['(1)  intra-group asset pooling arrangements of captive insurance or reinsurance undertakings may be exempted from the '] with score -5\n",
      "Processing line: ['calculation base referred to in Article 184(2) to the extent that there exist legally enforceable contractual terms '] with score -5\n",
      "Processing line: ['which ensure that the liabilities of the captive insurance or reinsurance undertaking will be offset by the intra-group '] with score -5\n",
      "Processing line: ['exposures it holds against other entities of the group. '] with score -5\n",
      "Processing line: ['(2)  the relative excess exposure threshold referred to in Article 184(1)(c) shall be equal to 15 % for the following single '] with score -5\n",
      "Processing line: ['name exposures: '] with score -5\n",
      "Processing line: ['(a)  exposures to credit institutions that do not belong to the same group and that have been assigned to the credit '] with score -5\n",
      "Processing line: ['quality step 2; '] with score -5\n",
      "Processing line: ['(b)  exposures to entities of the group that manages the cash of the captive insurance or reinsurance undertaking '] with score -5\n",
      "Processing line: ['that have been assigned to the credit quality step 2. '] with score -5\n",
      "Processing line: ['Article 107 '] with score -5\n",
      "Processing line: ['Simplified calculation of the risk mitigating effect for reinsurance arrangements or securitisation '] with score -5\n",
      "Processing line: ['1.'] with score -5\n",
      "Processing line: ['Where Article 88 is complied with, insurance or reinsurance undertakings may calculate the risk-mitigating effect '] with score -5\n",
      "Processing line: ['on underwriting risk of a reinsurance arrangement or securitisation referred to in Article 196 as follows: '] with score -5\n",
      "Processing line: ['RM', 're,all', ' �', 'Recoverables', 'i'] with score 13\n",
      "Processing line: ['Recoverables', 'all '] with score -5\n",
      "Processing line: ['where '] with score -5\n",
      "Processing line: ['(a)  ', 'RM', 're,all ', 'denotes the risk mitigating effect on underwriting risk of the reinsurance arrangements and securitisations for '] with score -2\n",
      "Processing line: ['all counterparties calculated in accordance with paragraph 2; '] with score -5\n",
      "Processing line: ['(b)  ', 'Recoverables', 'i ', 'denotes the best estimate of amounts recoverable from the reinsurance arrangement or securitisation '] with score -2\n",
      "Processing line: ['and the corresponding debtors for counterparty ', 'i ', 'and ', 'Recoverables', 'all ', 'denotes the best estimate of amounts recoverable '] with score -2\n",
      "Processing line: ['from the reinsurance arrangements and securitisations and the corresponding debtors for all counterparties. '] with score -5\n",
      "Processing line: ['2.'] with score -5\n",
      "Processing line: ['The risk mitigating effect on underwriting risk of the reinsurance arrangements and securitisations for all counter\\xad'] with score -5\n",
      "Processing line: ['parties referred to in paragraph 1 is the difference between the following capital requirements: '] with score -5\n",
      "Processing line: ['(a)  the hypothetical capital requirement for underwriting risk of the insurance or reinsurance undertaking if none of the '] with score -5\n",
      "Processing line: ['reinsurance arrangements and securitisations exist; '] with score -5\n",
      "Processing line: ['(b)  the capital requirements for underwriting risk of the insurance or reinsurance undertaking. '] with score -5\n",
      "Processing line: ['Article 108 '] with score -5\n",
      "Processing line: ['Simplified calculation of the risk mitigating effect for proportional reinsurance arrangements '] with score -5\n",
      "Processing line: ['Where Article 88 is complied with, insurance or reinsurance undertakings may calculate the risk-mitigating effect on '] with score -5\n",
      "Processing line: ['underwriting risk ', 'j ', 'of a proportional reinsurance arrangement for counterparty ', 'i ', 'referred to Article 196 as follows: '] with score -5\n",
      "Processing line: ['Recov', 'erables', 'i'] with score -5\n",
      "Processing line: ['BE', ' − Recov', 'erables', 'all'] with score -2\n",
      "Processing line: ['�', 'SCR', 'j '] with score 18\n",
      "Processing line: ['17.1.2015 '] with score -5\n",
      "Processing line: ['L 12/70 '] with score -5\n",
      "Processing line: ['Official Journal of the European Union '] with score -5\n",
      "Processing line: ['EN     '] with score -5\n",
      "Detected 2 equations.\n",
      "\n",
      "['RM', 're,all', ' �', 'Recoverables', 'i']\n",
      "--- Equation 1 ---\n",
      "  Bounding Box: (82.71662902832031, 498.2776794433594, 159.54339599609375, 514.3447875976562)\n",
      "  Reconstructed Text: RMre,all �Recoverablesi\n",
      "--------------------\n",
      "\n",
      "['�', 'SCR', 'j ']\n",
      "--- Equation 2 ---\n",
      "  Bounding Box: (155.28179931640625, 786.0509033203125, 177.87184143066406, 796.1041870117188)\n",
      "  Reconstructed Text: �SCRj \n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING THIS IS AI SLOP\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Heuristics Configuration ---\n",
    "\n",
    "# 1. Define characters that strongly indicate mathematical notation\n",
    "WEAK_MATH_SYMBOLS = {'×', '+', '−'}\n",
    "STRONG_MATH_SYMBOLS = {'√', '∑', '=', '¼', '∂', '∫', '≥', '≤', '≠', '�'}\n",
    "\n",
    "# 2. Define scoring weights for different features\n",
    "SCORING_WEIGHTS = {\n",
    "    'is_strong_math_symbol': 15,\n",
    "    'is_weak_math_symbol': 3,\n",
    "    'is_large_symbol': 3,\n",
    "    'is_subscript': 3,\n",
    "    'is_superscript': 3,\n",
    "    'is_italic': 1,\n",
    "    'is_largely_alphabetic': 5\n",
    "}\n",
    "\n",
    "# 3. Threshold for a line to be considered part of an equation\n",
    "LINE_SCORE_THRESHOLD = 9\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def merge_bboxes(bboxes):\n",
    "    \"\"\"Merges a list of bounding boxes into a single bounding box.\"\"\"\n",
    "    if not bboxes:\n",
    "        return None\n",
    "    min_x0 = min(b[0] for b in bboxes)\n",
    "    min_y0 = min(b[1] for b in bboxes)\n",
    "    max_x1 = max(b[2] for b in bboxes)\n",
    "    max_y1 = max(b[3] for b in bboxes)\n",
    "    return (min_x0, min_y0, max_x1, max_y1)\n",
    "\n",
    "def get_dominant_line_properties(line):\n",
    "    \"\"\"Calculates the most common font size and baseline for a line.\"\"\"\n",
    "    if not line['spans']:\n",
    "        return 0, 0, False\n",
    "        \n",
    "    baselines = [round(s['bbox'][3], 2) for s in line['spans']]\n",
    "    sizes = [round(s['size'], 2) for s in line['spans']]\n",
    "    fonts = [s['font'] for s in line['spans']]\n",
    "    \n",
    "    dominant_size = collections.Counter(sizes).most_common(1)[0][0]\n",
    "    dominant_baseline = collections.Counter(baselines).most_common(1)[0][0]\n",
    "    is_bold_dominant = 'Bold' in collections.Counter(fonts).most_common(1)[0][0]\n",
    "\n",
    "    return dominant_size, dominant_baseline, is_bold_dominant\n",
    "\n",
    "def is_likely_heading_or_prose(line, is_bold_dominant):\n",
    "    \"\"\"\n",
    "    Applies negative heuristics to determine if a line is likely a heading or regular text.\n",
    "    \"\"\"\n",
    "    full_text = \"\".join(span['text'] for span in line['spans']).strip()\n",
    "    \n",
    "    if not full_text:\n",
    "        return False\n",
    "\n",
    "    # Heuristic 1: Starts with a number like \"1.\" or \"A.\"\n",
    "    if re.match(r'^[\\[\\(]?\\d{1,2}[\\.\\)]', full_text):\n",
    "        return True\n",
    "\n",
    "    # Heuristic 2: Line is dominantly bold.\n",
    "    if is_bold_dominant:\n",
    "        return True\n",
    "\n",
    "    # Heuristic 3: High ratio of letters to other characters.\n",
    "    # Equations have a low ratio of letters.\n",
    "    text_no_space = full_text.replace(\" \", \"\")\n",
    "    if not text_no_space:\n",
    "        return False # Empty line\n",
    "        \n",
    "    alpha_chars = sum(1 for char in text_no_space if char.isascii()) #changed is isascii\n",
    "    total_chars = len(text_no_space)\n",
    "    alpha_ratio = alpha_chars / total_chars\n",
    "    \n",
    "    # If over 90% of characters are letters, it's likely prose/heading.\n",
    "    if alpha_ratio > 0.90:\n",
    "        return True\n",
    "    \n",
    "    # print(alpha_ratio)\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_positive_line_score(line, dominant_size, dominant_baseline):\n",
    "    line_score = 0\n",
    "    for span in line['spans']:\n",
    "                if any(char in WEAK_MATH_SYMBOLS for char in span['text']):\n",
    "                    line_score += SCORING_WEIGHTS['is_weak_math_symbol']\n",
    "                if any(char in STRONG_MATH_SYMBOLS for char in span['text']):\n",
    "                    line_score += SCORING_WEIGHTS['is_strong_math_symbol']\n",
    "                if 'Italic' in span['font']:\n",
    "                    line_score += SCORING_WEIGHTS['is_italic']\n",
    "                \n",
    "                height = span['bbox'][3] - span['bbox'][1]\n",
    "                if height > dominant_size * 1.5:\n",
    "                    line_score += SCORING_WEIGHTS['is_large_symbol']\n",
    "                \n",
    "                is_smaller = span['size'] < dominant_size * 0.9\n",
    "                span_baseline = span['bbox'][3]\n",
    "                \n",
    "                if is_smaller and span_baseline > dominant_baseline + 1:\n",
    "                    line_score += SCORING_WEIGHTS['is_subscript']\n",
    "                if is_smaller and span_baseline < dominant_baseline - 2:\n",
    "                    line_score += SCORING_WEIGHTS['is_superscript']\n",
    "\n",
    "    return line_score\n",
    "    \n",
    "\n",
    "\n",
    "# --- Main Detection Logic ---\n",
    "\n",
    "def detect_equations(page_data, verbose=0):\n",
    "    \"\"\"\n",
    "    Detects equations from a page's text dictionary representation.\n",
    "    \"\"\"\n",
    "    math_lines = []\n",
    "    \n",
    "    for block in page_data.get('blocks', []):\n",
    "        if block.get('type', 0) != 0:\n",
    "            continue\n",
    "            \n",
    "        for line in block.get('lines', []):\n",
    "            dominant_size, dominant_baseline, is_bold_dominant = get_dominant_line_properties(line)\n",
    "\n",
    "            if dominant_size == 0:\n",
    "                continue\n",
    "\n",
    "            line_score = calculate_positive_line_score(line, dominant_size, dominant_baseline)\n",
    "\n",
    "            # Check if the line matches heading/prose characteristics.\n",
    "            if is_likely_heading_or_prose(line, is_bold_dominant):\n",
    "                # continue # Skip this line, it's a false positive.\n",
    "                line_score -= SCORING_WEIGHTS['is_largely_alphabetic']\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing line: {[span['text'] for span in line['spans']]} with score {line_score}\")\n",
    "\n",
    "            if line_score >= LINE_SCORE_THRESHOLD:\n",
    "\n",
    "                math_lines.append({\n",
    "                    'score': line_score,\n",
    "                    'bbox': line['bbox'],\n",
    "                    'spans': line['spans']\n",
    "                })\n",
    "\n",
    "    # Stage 3: Clustering (no changes needed here)\n",
    "    if not math_lines:\n",
    "        return []\n",
    "\n",
    "    math_lines.sort(key=lambda l: l['bbox'][1])\n",
    "    clusters = []\n",
    "    current_cluster = [math_lines[0]]\n",
    "    \n",
    "    for i in range(1, len(math_lines)):\n",
    "        prev_line = current_cluster[-1]\n",
    "        current_line = math_lines[i]\n",
    "        vertical_gap = current_line['bbox'][1] - prev_line['bbox'][3]\n",
    "        prev_line_height = prev_line['bbox'][3] - prev_line['bbox'][1]\n",
    "        \n",
    "        if vertical_gap < prev_line_height * 0.5:\n",
    "            current_cluster.append(current_line)\n",
    "        else:\n",
    "            clusters.append(current_cluster)\n",
    "            current_cluster = [current_line]\n",
    "            \n",
    "    clusters.append(current_cluster)\n",
    "    \n",
    "    detected_equations = []\n",
    "    for cluster in clusters:\n",
    "        all_spans = [span for line in cluster for span in line['spans']]\n",
    "        cluster_bbox = merge_bboxes([line['bbox'] for line in cluster])\n",
    "        detected_equations.append({\n",
    "            'bbox': cluster_bbox,\n",
    "            'spans': all_spans\n",
    "        })\n",
    "        \n",
    "    return detected_equations\n",
    "\n",
    "def print_equations(equations):\n",
    "    print(f\"Detected {len(equations)} equations.\\n\")\n",
    "    \n",
    "    for i, eq in enumerate(equations):\n",
    "        eq_text = \"\".join([s['text'] for s in eq['spans']])\n",
    "\n",
    "        print([s['text'] for s in eq['spans']])\n",
    "        print(f\"--- Equation {i+1} ---\")\n",
    "        print(f\"  Bounding Box: {eq['bbox']}\")\n",
    "        print(f\"  Reconstructed Text: {eq_text}\")\n",
    "        print(\"-\" * 20 + \"\\n\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    # I have added a heading to the sample data to test the new filter.\n",
    "import os\n",
    "# for filename in os.listdir(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\")):\n",
    "# with open(os.path.join(\"data\", \"raw\", \"test-data\", \"pymupdf-dict-repr\", filename), 'r') as f:\n",
    "#     sample_page_data = json.load(f)\n",
    "\n",
    "with open(os.path.join(\"data\", \"raw\", \"solvency-II-files\", \"solvency II - level 2.pdf\"), 'rb') as f:\n",
    "    original_doc = pymupdf.open(f)\n",
    "\n",
    "page_number = 70\n",
    "\n",
    "equations = detect_equations(original_doc[page_number - 1].get_text(\"dict\", flags=0), verbose=1)\n",
    "\n",
    "# print(f\"File: {filename}\")\n",
    "print_equations(equations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a755e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     test[\u001b[33m\"\u001b[39m\u001b[33mspans\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: equation_span, \u001b[33m\"\u001b[39m\u001b[33mfont\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mItalic\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m      7\u001b[39m is_likely_heading_or_prose(test, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mcalculate_positive_line_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 'ð'.isascii()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mcalculate_positive_line_score\u001b[39m\u001b[34m(line, dominant_size, dominant_baseline)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mItalic\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m span[\u001b[33m'\u001b[39m\u001b[33mfont\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     94\u001b[39m     line_score += SCORING_WEIGHTS[\u001b[33m'\u001b[39m\u001b[33mis_italic\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m height = \u001b[43mspan\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbbox\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m3\u001b[39m] - span[\u001b[33m'\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m height > dominant_size * \u001b[32m1.5\u001b[39m:\n\u001b[32m     98\u001b[39m     line_score += SCORING_WEIGHTS[\u001b[33m'\u001b[39m\u001b[33mis_large_symbol\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'bbox'"
     ]
    }
   ],
   "source": [
    "# equation_spans = \"*Lapse*\" \"*up* ¼ 0,5 � *l* *up* � *n* *up* � *S* *up*\n",
    "equation_spans = [\"RM\", \"re,all\", \"�\", \"Recoverables\", \"[Recoverables]\", \"all\", \"[i]\"]\n",
    "test = {\"spans\": []}\n",
    "for equation_span in equation_spans:\n",
    "    test[\"spans\"].append({\"text\": equation_span, \"font\": 'Italic'})\n",
    "\n",
    "# is_likely_heading_or_prose(test, False)\n",
    "# calculate_positive_line_score(test, 12, 5)\n",
    "# 'ð'.isascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fbf0e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 equations.\n",
      "\n",
      "['CorrEQ', 'ð', 'r,s', 'Þ', ' �', 'SCR', 'ð', 'earthquake,r', 'Þ', ' �', 'SCR', 'ð', 'earthquake,s', 'Þ', 'Þ þ', ' SCR', '2', 'SCR', 'earthquake', ' ¼']\n",
      "--- Equation 1 ---\n",
      "  Bounding Box: (82.716064453125, 240.76597595214844, 337.27325439453125, 251.62977600097656)\n",
      "  Reconstructed Text: CorrEQðr,sÞ �SCRðearthquake,rÞ �SCRðearthquake,sÞÞ þ SCR2SCRearthquake ¼\n",
      "--------------------\n",
      "\n",
      "['L', 'ð', 'earthquake,r', 'Þ', ' ¼', ' Q', 'ð', 'earthquake,r', 'Þ', ' �', 'Corr', 'ð', 'earthquake,r,i,j', 'Þ', ' �', 'WSI', 'ð', 'earthquake,r,i', 'Þ', ' �', 'WSI', 'ð', 'earthquake,r,j', 'Þ']\n",
      "--- Equation 2 ---\n",
      "  Bounding Box: (82.7159423828125, 418.3394470214844, 359.936279296875, 428.738525390625)\n",
      "  Reconstructed Text: Lðearthquake,rÞ ¼ Qðearthquake,rÞ �Corrðearthquake,r,i,jÞ �WSIðearthquake,r,iÞ �WSIðearthquake,r,jÞ\n",
      "--------------------\n",
      "\n",
      "['WSI', 'ð', 'earthquake,r,i', 'Þ', ' ¼', ' W', 'ð', 'earthquake,r,i', 'Þ', ' �', 'SI', 'ð', 'earthquake,r,i', 'Þ']\n",
      "--- Equation 3 ---\n",
      "  Bounding Box: (82.71573638916016, 588.3056640625, 231.7544403076172, 598.647216796875)\n",
      "  Reconstructed Text: WSIðearthquake,r,iÞ ¼ Wðearthquake,r,iÞ �SIðearthquake,r,iÞ\n",
      "--------------------\n",
      "\n",
      "['SI', 'ð', 'earthquake,r,i', 'Þ', ' ¼', ' SI', 'ð', 'property,r,i', 'Þ', ' þ', ' SI', 'ð', 'onshore-property,r,i', 'Þ']\n",
      "--- Equation 4 ---\n",
      "  Bounding Box: (82.71598815917969, 689.275146484375, 233.70298767089844, 699.6738891601562)\n",
      "  Reconstructed Text: SIðearthquake,r,iÞ ¼ SIðproperty,r,iÞ þ SIðonshore-property,r,iÞ\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "# 1. how to ensure that replacement is correct?\n",
    "# 2. how to replace the exact text?\n",
    "\n",
    "equations_detected = None\n",
    "with open(os.path.join(\"data\", \"raw\", \"test-data\", \"solvency II - level 2 - 78 - replacement-test.pdf\"), 'rb') as f:\n",
    "    doc = pymupdf.open(f)\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\", flags=0)[\"blocks\"]\n",
    "        equations = detect_equations({\"blocks\": blocks})\n",
    "        # tables = page.find_tables()\n",
    "        # pprint.pprint(tables)\n",
    "        print_equations(equations)\n",
    "        # page.get_pixmap(dpi=1200).save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))\n",
    "        equations_detected = equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820ae78",
   "metadata": {},
   "source": [
    "# Research on regex patterns to find equation on page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_1 = \"\"\"\n",
    "1. The capital requirement for earthquake risk shall be equal to the following:\n",
    "\n",
    "\n",
    "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n",
    "*SCR* *earthquake* ¼ ðX *CorrEQ* ð *r,s* Þ � *SCR* ð *earthquake,r* Þ � *SCR* ð *earthquake,s* Þ Þ þ *SCR* [2] ð *earthquake,other* Þ\n",
    "\n",
    "\n",
    "s\n",
    "\n",
    "\n",
    "*CorrEQ* ð *r,s* Þ � *SCR* ð *earthquake,r* Þ � *SCR* ð *earthquake,s* Þ Þ þ *SCR* [2] ð *earthquake,other* Þ\n",
    "\n",
    "\n",
    "ð *r,s* Þ\n",
    "\n",
    "\n",
    "where:\n",
    "\"\"\"\n",
    "snippet_2 = \"\"\"\n",
    "following amount:\n",
    "\n",
    "\n",
    "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n",
    "*L* ð *earthquake,r* Þ ¼ *Q* ð *earthquake,r* Þ � X *Corr* ð *earthquake,r,i,j* Þ � *WSI* ð *earthquake,r,i* Þ � *WSI* ð *earthquake,r,j* Þ\n",
    "\n",
    "\n",
    "s\n",
    "\n",
    "\n",
    "*Corr* ð *earthquake,r,i,j* Þ � *WSI* ð *earthquake,r,i* Þ � *WSI* ð *earthquake,r,j* Þ\n",
    "\n",
    "\n",
    "ð *i,j* Þ\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2d13d82a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'snippet_window' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 270\u001b[39m\n\u001b[32m    267\u001b[39m     markdown_file = md_file.read()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# markdown_file = pymupdf4llm.to_markdown(original_doc) # can be changed to own preprocessed markdown_file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mmatch_equations\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkdown_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_output_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mmatch_equations\u001b[39m\u001b[34m(original_doc, markdown_file, process_page_mask, std_output_file)\u001b[39m\n\u001b[32m    178\u001b[39m pprint.pprint(count_in_expected_order, stream=std_output_file)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# raise Exception(\"Not accepted match\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43mevaluate_placement_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation_spans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnippets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_output_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd_output_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<End of equation NOT_ACCEPTED >\u001b[39m\u001b[33m\"\u001b[39m, file=std_output_file)\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# total_matches_not_accepted += 1\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mevaluate_placement_likelihood\u001b[39m\u001b[34m(ordered_equation_spans, snippets, verbose, std_output_file)\u001b[39m\n\u001b[32m     66\u001b[39m         found_pos = -\u001b[32m1\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWINDOW \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msnippet_window\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspan\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, file=std_output_file)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m found_pos != -\u001b[32m1\u001b[39m:\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspan\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfound_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in snippet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, file=std_output_file)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'snippet_window' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "regex_without_page = r\"((following|follows)[^\\n]*?:)((?!following|follows).)*?(?=\\n(where|provided))\"\n",
    "# example of page within (following|follows) ... where block --- Page 78 --- in solvency II - level 2\n",
    "regex_with_page = r'((following|follows)[^\\n]*?:).*?(--- Page \\d+ ---).*?(?=where)'\n",
    "\n",
    "def find_equation_snippets(markdown_file):\n",
    "    matches = re.finditer(regex_without_page, markdown_file, re.DOTALL)\n",
    "    return [match.group(0) for match in matches]\n",
    "\n",
    "# def handle_replacement():\n",
    "#     regex = \"|\".join([regex_without_page, regex_with_page])\n",
    "\n",
    "#     # page = response.text()\n",
    "#     # with open(os.path.join(\"data\", \"preprocessed-step-final\", \"solvency-II-files\", \"final_solvency II - level 2.pdf.md\"), 'r', encoding=\"utf-8\") as f:\n",
    "#     #     page = f.read()\n",
    "\n",
    "#     matches = re.finditer(regex, page, re.DOTALL)\n",
    "\n",
    "#     # print(\"Found {} matches\".format(len(matches)))\n",
    "#     for match in matches:\n",
    "#         # print(match)\n",
    "#         print(match.group(0))\n",
    "#         if match.group(2) and match:\n",
    "#             # match without page\n",
    "#             print(\"Match without page: \", match.group(2), match.group(3))\n",
    "#         else:\n",
    "#             print(\"Match with page: \", match.group(1))\n",
    "#         # # replace equation\n",
    "\n",
    "\n",
    "def evaluate_placement_likelihood(ordered_equation_spans, snippets, verbose=0, std_output_file=None):\n",
    "    count_per_snippet = [0] * len(snippets)\n",
    "    count_in_expected_order = [0] * len(snippets)\n",
    "    for i, snippet in enumerate(snippets):\n",
    "        # snippet = snippet.replace(\" \", \"\")\n",
    "        # snippet = snippet.replace(\"*\", \"\")\n",
    "        snippet = snippet.replace(\"*\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\n\", \"\")\n",
    "        snippet_chunks = snippet.split(\" \")\n",
    "        if verbose:\n",
    "            print(f\"ordered equation_spans: {ordered_equation_spans}\", file=std_output_file)\n",
    "            print(f\"evaluation for snippet {i}: {snippet}\", file=std_output_file)\n",
    "        current_search_index = 0 # tries to take the order into account\n",
    "        current_window_extra_size = 0\n",
    "        for idx, span in enumerate(ordered_equation_spans):\n",
    "            span = span.replace(\" \", \"\")\n",
    "            # set up found_pos and search_window\n",
    "            if idx == 0 or current_search_index == 0: # find first snippet with match\n",
    "                # found_pos = snippet.find(span)\n",
    "                try: \n",
    "                    found_pos = snippet_chunks.index(span)\n",
    "                except ValueError:\n",
    "                    found_pos = -1\n",
    "            else:\n",
    "                # SEARCH_WINDOW = len(span) * 3 if len(span) * 3 < len(snippet) else len(span)\n",
    "                # end = current_search_index + SEARCH_WINDOW + 1 + current_window_extra_size\n",
    "                # end = min(end, len(snippet))  # Ensure we don't go out of bounds\n",
    "                # found_pos = snippet.find(span, current_search_index, end)\n",
    "                SEARCH_WINDOW = current_window_extra_size + current_window_extra_size + 3 if current_search_index + current_window_extra_size + 3 < len(snippet_chunks) else len(snippet_chunks)\n",
    "                snippet_window = snippet_chunks[current_search_index:current_search_index + SEARCH_WINDOW]\n",
    "                try:\n",
    "                    found_pos = current_search_index + snippet_window.index(span)\n",
    "                except ValueError:\n",
    "                    found_pos = -1\n",
    "            if verbose:\n",
    "                print(f\"WINDOW {snippet_window} for {span}\", file=std_output_file)\n",
    "                if found_pos != -1:\n",
    "                    print(f\"Found '{span}' at position {found_pos} in snippet {i}.\", file=std_output_file)\n",
    "\n",
    "            # update score accordingly\n",
    "            if span in snippet_chunks:\n",
    "                # print(f\"Found '{span}' in snippet {i}.\")\n",
    "                count_per_snippet[i] += 1\n",
    "                if found_pos != -1:\n",
    "                    count_in_expected_order[i] += 1\n",
    "                    current_search_index = found_pos\n",
    "                    current_window_extra_size = 0\n",
    "                else:\n",
    "                    current_window_extra_size += 1\n",
    "    return count_per_snippet, count_in_expected_order\n",
    "\n",
    "def find_page(markdown_file, page_number, is_last_page):\n",
    "    page_regex = r\"--- Page \" + str(page_number) + \" ---.*?\"\n",
    "    if not is_last_page:\n",
    "        #certain page will have the 'where' directly on the next page:\n",
    "        page_regex += r\"(--- Page \" + str(page_number+1) + \" ---)(\\nwhere)?\"\n",
    "    else:\n",
    "        print(\"LAST PAGE\")\n",
    "    \n",
    "    match = re.search(page_regex, markdown_file, re.DOTALL)\n",
    "    if not match:\n",
    "        raise Exception(f\"Expected exactly one match for page {page_number}, found 0.\")\n",
    "    return match.group(0)\n",
    "\n",
    "def match_equations(original_doc, markdown_file, process_page_mask, std_output_file):\n",
    "    # markdown_file = pymupdf4llm.to_markdown(doc)\n",
    "    page_numbers_unresolved = []\n",
    "    equation_matching_unresolved = []\n",
    "    total_number_of_pages_with_equations = 0\n",
    "    total_matches_not_accepted = 0\n",
    "    total_equation_detected = 0\n",
    "    total_equation_matched = 0\n",
    "    total_equation_matched_accepted = 0\n",
    "    total_equation_matched_with_duplicate_match = 0\n",
    "    total_not_enough_snippets = 0\n",
    "\n",
    "    last_page_index = 0\n",
    "\n",
    "    for i in range(len(mask) - 1, -1, -1):\n",
    "        if mask[i]:\n",
    "            last_page_index = i\n",
    "            break\n",
    "\n",
    "    for page in original_doc:\n",
    "        if process_page_mask[page.number] == 0:\n",
    "            # print(f\"Skipping page {page.number + 1} (masked)\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"page with {page.number + 1} is being processed\")\n",
    "        page_text_in_dict = page.get_text(\"dict\", flags=0)\n",
    "        # detect_equation will look if the page has equations\n",
    "        equations_detected = detect_equations(page_text_in_dict)\n",
    "        total_equation_detected += len(equations_detected)\n",
    "        # find_snippets will try to identify the exact replacement area\n",
    "        # based on following: where regex\n",
    "\n",
    "        if equations_detected:\n",
    "            print(f\"*** Found {len(equations_detected)} equations on page {page.number + 1}. ***\", file=std_output_file)\n",
    "            total_number_of_pages_with_equations += 1\n",
    "\n",
    "            # the original doc will not be changed, the snippets correspond the current representation of the original document in markdown.\n",
    "            page_md = find_page(markdown_file, page.number + 1, last_page_index == page.number)\n",
    "            snippets = find_equation_snippets(page_md)\n",
    "\n",
    "            \n",
    "            if len(snippets) < len(equations_detected):\n",
    "                # not implemented yet\n",
    "                print(f\"[ERROR, NOT ENOUGH SNIPPETS]: Found {len(snippets)}, expected {len(equations_detected)}.\", file=std_output_file)\n",
    "                for eq in equations_detected:\n",
    "                    equation_text = [s['text'] for s in eq['spans']]\n",
    "                    print(f\"Equations text: {equation_text}\", file=std_output_file)\n",
    "                    # print(f\"Equations obj: {eq}\", file=std_output_file)\n",
    "                # raise Exception(f\"Not enough snippets found. Found {len(snippets)}, expected {len(equations_detected)}.\")\n",
    "                page_numbers_unresolved.append(page.number + 1)\n",
    "                total_not_enough_snippets += len(equations_detected)\n",
    "                continue\n",
    "                \n",
    "            best_matches_indexes = []\n",
    "            for idx, eq in enumerate(equations_detected):\n",
    "                equation_spans = [s['text'] for s in eq['spans']]\n",
    "                # print(f\"\\n*** Evaluating spans from equation {idx + 1}: {spans} ***\")\n",
    "\n",
    "                count_per_snippet, count_in_expected_order = evaluate_placement_likelihood(equation_spans, snippets)\n",
    "                # print(f\"Count per snippet: {count_per_snippet}\")\n",
    "                # max_index, max_count = max(enumerate(count_per_snippet), key=lambda x: x[1])\n",
    "                # max_index_in_expected_order, max_in_expected_order = max(enumerate(count_in_expected_order), key=lambda x: x[1])\n",
    "\n",
    "                highest_score = 0\n",
    "                best_match_index = 0\n",
    "                snippet_scores = []\n",
    "                for i, snippet in enumerate(snippets):\n",
    "                    snippet_score = count_per_snippet[i] + 2 * count_in_expected_order[i]\n",
    "                    snippet_scores.append(snippet_score)\n",
    "                    if snippet_score > highest_score:\n",
    "                        highest_score = snippet_score\n",
    "                        best_match_index = i\n",
    "                    # elif snippet_score == highest_score:\n",
    "                        # raise Exception(\"A tie, multiple snippets have the same score.\")\n",
    "                COUNT_ERROR_ACCEPTANCE = 0.8 #the portion of spans that must be matched\n",
    "                ORDER_ERROR_ACCEPTANCE = 0.25\n",
    "                if len(equation_spans) * ORDER_ERROR_ACCEPTANCE + len(equation_spans) * COUNT_ERROR_ACCEPTANCE > highest_score:\n",
    "                    print(f\"<Equation {idx + 1} on page {page.number + 1} did not match sufficiently with any snippet.>\", file=std_output_file)\n",
    "                    print(f\"Did not meet total_score requirement, potentially {count_per_snippet[best_match_index]} / {len(equation_spans) * COUNT_ERROR_ACCEPTANCE} in snippet {best_match_index + 1}.\", file=std_output_file)\n",
    "                    pprint.pprint(count_per_snippet, stream=std_output_file)\n",
    "                    print(f\"Did not meet total_score requirement, potentially {count_in_expected_order[best_match_index]} / {len(equation_spans) * ORDER_ERROR_ACCEPTANCE} in snippet {best_match_index + 1}.\", file=std_output_file)\n",
    "                    pprint.pprint(count_in_expected_order, stream=std_output_file)\n",
    "                    # raise Exception(\"Not accepted match\")\n",
    "                    evaluate_placement_likelihood(equation_spans, snippets, verbose=1, std_output_file=std_output_file)\n",
    "                    print(f\"<End of equation NOT_ACCEPTED >\", file=std_output_file)\n",
    "                    # total_matches_not_accepted += 1\n",
    "                    best_match_index = \"NOT_ACCEPTED\"\n",
    "                    \n",
    "                best_matches_indexes.append(best_match_index)\n",
    "                # print(f\"SNIPPET SCORES: {snippet_scores}\")\n",
    "                # print(f\"BEST MATCH FOR EQUATION IS: {best_match_index + 1 if isinstance(best_match_index, int) else best_match_index} with score {highest_score}.\")\n",
    "                # print(f\"*** END OF Evaluating spans from equation {idx + 1} ***\\n\")\n",
    "\n",
    "            total_equation_matched += len(best_matches_indexes)\n",
    "                \n",
    "            if \"NOT_ACCEPTED\" in best_matches_indexes:\n",
    "                page_numbers_unresolved.append(page.number + 1)\n",
    "                total_matches_not_accepted += len(best_matches_indexes)\n",
    "                print(f\"[ERROR, MATCHES NOT ACCEPTED] SKIPPING PAGE {page.number + 1}, {best_matches_indexes.count('NOT_ACCEPTED')}...\", file=std_output_file)\n",
    "                continue\n",
    "            elif len(set(best_matches_indexes)) != len(best_matches_indexes):\n",
    "                print(\"[ERROR, MATCHED MULTIPLE TIMES] \", file=std_output_file)\n",
    "                for eq in equations_detected:\n",
    "                    equation_text = [s['text'] for s in eq['spans']]\n",
    "                    print(f\"Equations text: {equation_text}\", file=std_output_file)\n",
    "                for idx, snippet in enumerate(snippets):\n",
    "                    print(f\"*** MATCH FOR SNIPPET {idx + 1} ***\", file=std_output_file)\n",
    "                    print(f\"Snippet {idx + 1}:\\n{snippet}\", file=std_output_file)\n",
    "                    print(f\"Best match for snippet {idx + 1}:\\n{[s['text'] for s in equations_detected[best_matches_indexes[idx]]['spans']] if idx < len(best_matches_indexes) else 'None'}\\n\", file=std_output_file)\n",
    "                    print(f\"*** END OF BEST MATCH OF SNIPPET {idx + 1} ***\", file=std_output_file)\n",
    "\n",
    "                print(f\"Best matches: {best_matches_indexes}\", file=std_output_file)\n",
    "                equation_matching_unresolved.append((page.number + 1, best_matches_indexes))\n",
    "                page_numbers_unresolved.append(page.number + 1)\n",
    "                total_equation_matched_with_duplicate_match += len(best_matches_indexes)\n",
    "                continue\n",
    "                # raise Exception(\"equation matched with multiple\")\n",
    "\n",
    "            total_equation_matched_accepted += len(best_matches_indexes)\n",
    "            # print(f\"Equation text: {''.join([s['text'] for s in eq['spans']])}\")\n",
    "        # tables = page.find_tables()\n",
    "        # pprint.pprint(tables)\n",
    "        # print_equations(equations_detected)\n",
    "        # page.get_pixmap(dpi=1200).save(\"data/raw/test-data/solvency_II_level_1_v2_equations_page_{}.png\".format(page.number))\n",
    "            print(f\"[ACCEPTED] page was matched\", file=std_output_file)\n",
    "            print(f\"*** End of page {page.number + 1}. ***\", file=std_output_file)\n",
    "\n",
    "\n",
    "    print(f\"Total equations detected: {total_equation_detected}\",)\n",
    "    print(f\"Total not enough snippets: {total_not_enough_snippets}/{total_equation_detected}\")\n",
    "    print(f\"Total equations matched: {total_equation_matched}/{total_equation_detected}\")\n",
    "    print(f\"Total matches accepted: {total_equation_matched_accepted}/{total_equation_matched}\")\n",
    "    print(f\"Total matches not accepted: {total_matches_not_accepted}/{total_equation_matched}\")\n",
    "    print(f\"Total matches with duplicate match: {total_equation_matched_with_duplicate_match}/{total_equation_matched}\")\n",
    "    print(f\"Unresolved {len(page_numbers_unresolved)} of {total_number_of_pages_with_equations} pages: {page_numbers_unresolved}\")\n",
    "    print(f\"Unresolved matches: {equation_matching_unresolved} \")\n",
    "    \n",
    "\n",
    "# test_file = \"solvency II - level 2 - 1-295.pdf\"\n",
    "\n",
    "\n",
    "# will not be updated. for testing only\n",
    "copy_structure_solvency_II_level_2 ={\n",
    "  \"file_name\": \"solvency II - level 2.pdf\",\n",
    "  \"toc\": (1, 4),\n",
    "  \"recitals\": (5, 20),\n",
    "  \"CORRELATION_TABLES\": (296, 797),\n",
    "  \"NUM_TITLES\": 3,\n",
    "  \"NUM_CHAPTERS\": 25,\n",
    "  \"NUM_SECTIONS\": 61,\n",
    "  \"NUM_SUBSECTIONS\": 31,\n",
    "  \"NUM_ARTICLES\": 381\n",
    "}\n",
    "\n",
    "with open(\"print-output.txt\", \"w\", encoding=\"utf-8\") as std_output_file:\n",
    "    with open(os.path.join(\"data\", \"raw\", \"solvency-II-files\", \"solvency II - level 2.pdf\"), 'rb') as f:\n",
    "        original_doc = pymupdf.open(f) # detection on original pdf\n",
    "        # markdown_file = pymupdf4llm.to_markdown(original_doc) # can be changed to own preprocessed markdown_file\n",
    "        mask : list = [1] * original_doc.page_count\n",
    "\n",
    "        exceptions = [copy_structure_solvency_II_level_2[\"toc\"], copy_structure_solvency_II_level_2[\"CORRELATION_TABLES\"]]\n",
    "\n",
    "        for start, end in exceptions:\n",
    "            for i in range(start - 1, end): #page delimiters are one-indexed\n",
    "                if i >= original_doc.page_count:\n",
    "                    raise Exception(f\"Page index {i} out of bounds for document with {original_doc.page_count} pages.\")\n",
    "                mask[i] = 0\n",
    "\n",
    "\n",
    "        with open(os.path.join(\"data\", \"preprocessed-step-2\", \"solvency-II-files\", \"substep-5-solvency II - level 2.pdf.md\"), 'r', encoding=\"utf-8\") as md_file:\n",
    "            markdown_file = md_file.read()\n",
    "\n",
    "        # markdown_file = pymupdf4llm.to_markdown(original_doc) # can be changed to own preprocessed markdown_file\n",
    "        match_equations(original_doc, markdown_file, mask, std_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\", \n",
    "        api_key=os.environ[\"GOOGLE_API_KEY\"], \n",
    "        temperature=0.2,\n",
    ")\n",
    "\n",
    "page_number = 0\n",
    "test_image = \"data/raw/test-data/solvency_II_equations_page_{}.png\".format(page_number)\n",
    "\n",
    "with open(test_image, \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        message = HumanMessage(\n",
    "        content=[\n",
    "                {\"type\": \"text\", \"text\": \"Extract equation(s) in latex from this page, only consider full equation, not inline reference to variable\"},\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{image_data}\"},\n",
    "                },\n",
    "        ]\n",
    "        )\n",
    "        response = llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31d4764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] likely a success\n",
      "Match is:  \n",
      "\\text{SCR}_{\\text{earthquake}} = \\sqrt{\\sum_{(r,s)} \\text{CorrEQ}_{(r,s)} \\cdot \\text{SCR}_{(\\text{earthquake},r)} \\cdot \\text{SCR}_{(\\text{earthquake},s)} + \\text{SCR}_{(\\text{earthquake,other})}^{2}}\n",
      "\n",
      "Match is:  \n",
      "L_{(\\text{earthquake},r)} = Q_{(\\text{earthquake},r)} \\cdot \\sqrt{\\sum_{(i,j)} \\text{Corr}_{(\\text{earthquake},r,i,j)} \\cdot \\text{WSI}_{(\\text{earthquake},r,i)} \\cdot \\text{WSI}_{(\\text{earthquake},r,j)}}\n",
      "\n",
      "Match is:  \n",
      "\\text{WSI}_{(\\text{earthquake},r,i)} = W_{(\\text{earthquake},r,i)} \\cdot \\text{SI}_{(\\text{earthquake},r,i)}\n",
      "\n",
      "Match is:  \n",
      "\\text{SI}_{(\\text{earthquake},r,i)} = \\text{SI}_{(\\text{property},r,i)} + \\text{SI}_{(\\text{onshore-property},r,i)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from markdown_it import MarkdownIt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "md = MarkdownIt()\n",
    "\n",
    "# last step\n",
    "equations_converted = re.findall(r\"```latex(.*?)```\", response.text(), re.DOTALL)\n",
    "\n",
    "if len(equations_converted) == len(equations_detected):\n",
    "    print(\"[Success] likely a success\")\n",
    "else:\n",
    "    print(\"[Warning] page requires additional review\")\n",
    "\n",
    "for match in equations_converted:\n",
    "    # latex = match.group(1)\n",
    "    # Do something with the extracted LaTeX\n",
    "    # md.render(match)\n",
    "    # print(HTML(md.render(match)))\n",
    "    print(\"Match is: \", match)\n",
    "    # display(HTML(md.render(match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d07a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if I just detec them and replace the entire page\n",
    "# e.g. send it all to unstructured or mathpix\n",
    "# or a multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47eb3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d717a151",
   "metadata": {},
   "source": [
    "# Integration into existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3408f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question on interference. I need access \n",
    "\n",
    "# Integration\n",
    "# instead of doc[0].get_text() you can just use the langchain.Document.page_content\n",
    "# but the problem is you need the doc[0].get_text(\"dict\", flags=0) representation as well\n",
    "# therefore preferably I get access to the pymupdf.Document in the pymupdf4llm code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f4a0f",
   "metadata": {},
   "source": [
    "# Exploring other solvency II-files\n",
    "The solvency II - level 2 regulation seems to be the most heavy on equations (~100). However, we will also analyse if other files have many equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49587516",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cefc3ac",
   "metadata": {},
   "source": [
    "with open(\"print-output.txt\", \"w\", encoding=\"utf-8\") as std_output_file:\n",
    "    with open(os.path.join(\"data\", \"raw\", \"test-data\", \"equation-examples\", test_file), 'rb') as f:\n",
    "        match_equations(f, std_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regulation-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
